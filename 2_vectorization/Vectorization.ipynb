{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the vocabulary gathered on the semantic scholar dataset to vectorize the data.\n",
    "\n",
    "- The vocabulary is about 1 million terms\n",
    "- we don't actually need a 1 million x 2 million matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "sys.path.append(\"../../tools\")\n",
    "import my_stopwords3\n",
    "\n",
    "from sys import getsizeof\n",
    "\n",
    "stop = my_stopwords3.get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pickle.load(open(\"vocabulary.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981 1.953984260559082\n",
      "1982 2.189453363418579\n",
      "1983 2.5410115718841553\n",
      "1984 2.602524518966675\n",
      "1985 3.117190361022949\n",
      "1986 3.5410685539245605\n",
      "1987 4.0352842807769775\n",
      "1988 4.608623504638672\n",
      "1989 5.237196207046509\n",
      "1990 7.498921155929565\n",
      "1991 7.936845541000366\n",
      "1992 8.258606433868408\n",
      "1993 9.754759311676025\n",
      "1994 11.668789625167847\n",
      "1995 13.430156946182251\n",
      "1996 12.849214315414429\n",
      "1997 14.286081790924072\n",
      "1998 16.66336727142334\n",
      "1999 17.9203839302063\n"
     ]
    }
   ],
   "source": [
    "years = list(range(1981, 2021))\n",
    "vectors = []\n",
    "for year in years:\n",
    "    t0 = time.time()\n",
    "    with open(\"../../Data/semantic_scholar_cleaned_langdetect/\"+str(year)+\".txt\", \"r\") as f:\n",
    "        documents = f.readlines()\n",
    "        documents = [d.strip() for d in documents] \n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             ngram_range=(1,4),\n",
    "                             vocabulary=vocabulary,\n",
    "                             stop_words=stop\n",
    "                            )\n",
    "    \n",
    "    vector = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    del documents\n",
    "    \n",
    "    vector[vector>1] = 1\n",
    "    summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "    \n",
    "    del vector\n",
    "    \n",
    "    vectors.append(summed)\n",
    "    \n",
    "    del summed\n",
    "    \n",
    "    print(year, time.time()-t0)\n",
    "    \n",
    "# Turn the vector into a pandas dataframe\n",
    "# This has to happen because list indexing is waaaaaay too slow\n",
    "df = pd.DataFrame(vectors, columns=vocabulary) \n",
    "df.index = years\n",
    "pickle.dump(df, open(\"../stacked_vectors/semantic_scholar.p\", \"wb\"))\n",
    "\n",
    "del vectors\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get year sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1981, 2021))\n",
    "document_count_per_year = []\n",
    "for year in years:\n",
    "    t0 = time.time()\n",
    "    with open(\"../../Data/semantic_scholar_cleaned_langdetect/\"+str(year)+\".txt\", \"r\") as f:\n",
    "        documents = f.readlines()\n",
    "        documents = [d.strip() for d in documents]\n",
    "        document_count_per_year.append(len(documents))\n",
    "    \n",
    "df = pd.DataFrame(document_count_per_year, columns=['documents']) \n",
    "df.index = years\n",
    "pickle.dump(df, open(\"../stacked_vectors/semantic_scholar_document_count.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USPTO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201801 80.6875684261322\n",
      "201802 69.93550062179565\n",
      "201803 87.11116528511047\n",
      "201804 70.86444330215454\n",
      "201805 73.76353549957275\n",
      "201806 77.17390489578247\n",
      "201807 71.7418704032898\n",
      "201808 74.30070400238037\n",
      "201809 66.70476150512695\n",
      "201810 68.35228276252747\n",
      "201811 64.8125205039978\n",
      "201812 156.31264281272888\n",
      "201901 57.26950144767761\n",
      "201902 48.47320866584778\n",
      "201903 55.42344832420349\n",
      "201904 52.347954511642456\n",
      "201905 48.598304271698\n",
      "201906 41.65903568267822\n",
      "201907 50.00160551071167\n",
      "201908 33.42923831939697\n",
      "201909 29.382107257843018\n",
      "201910 26.390461921691895\n",
      "201911 21.33091115951538\n",
      "201912 119.59065771102905\n",
      "202001 14.68824315071106\n",
      "202002 15.876067399978638\n",
      "202003 10.64989447593689\n",
      "202004 8.24580979347229\n",
      "202005 6.135050296783447\n",
      "202006 5.312021017074585\n",
      "202007 3.696535348892212\n",
      "202008 1.9552507400512695\n",
      "202009 1.5853545665740967\n",
      "202010 1.0329370498657227\n",
      "202011 0.7327725887298584\n",
      "202012 92.4698874950409\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for y in range(2018, 2021):\n",
    "    vectors = []\n",
    "    files = []\n",
    "    for m in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "        filename = str(y)+m\n",
    "        files.append(filename)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        with open(\"../../Data/uspto_cleaned/\"+str(filename)+\".txt\", \"r\") as f:\n",
    "            documents = f.readlines()\n",
    "            documents = [d.strip() for d in documents]\n",
    "\n",
    "        vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                 ngram_range=(1,4),\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words=stop\n",
    "                                )\n",
    "\n",
    "        vector = vectorizer.fit_transform(documents)\n",
    "\n",
    "        del documents\n",
    "\n",
    "        vector[vector>1] = 1\n",
    "        summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "\n",
    "        del vector\n",
    "\n",
    "        vectors.append(summed)\n",
    "\n",
    "        del summed\n",
    "        \n",
    "        if m=='12':\n",
    "            # Turn the vector into a pandas dataframe\n",
    "            # This has to happen because list indexing is waaaaaay too slow\n",
    "            df = pd.DataFrame(vectors, columns=vocabulary) \n",
    "            df.index = files\n",
    "            pickle.dump(df, open(\"../stacked_vectors/uspto_\"+str(y)+\".p\", \"wb\"))            \n",
    "            \n",
    "            del df\n",
    "            del vectors \n",
    "        print(filename, time.time()-t0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splice these stacked vectors into one vector**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uspto = pickle.load(open(\"../stacked_vectors/uspto_2006.p\", \"rb\"))\n",
    "for year in range(2007,2021):\n",
    "    new = pickle.load(open(\"../stacked_vectors/uspto_\"+str(year)+\".p\", \"rb\"))\n",
    "    uspto = pd.concat([uspto, new])\n",
    "    \n",
    "pickle.dump(uspto, open(\"../stacked_vectors/uspto.p\", \"wb\"))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get doc count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count_per_year = []\n",
    "years = list(range(2006,2021))\n",
    "\n",
    "for y in range(2006, 2021):\n",
    "    count = 0\n",
    "    for m in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "        filename = str(y)+m\n",
    "        with open(\"../../Data/uspto_cleaned/\"+str(filename)+\".txt\", \"r\") as f:\n",
    "            documents = f.readlines()\n",
    "            documents = [d.strip() for d in documents]\n",
    "            count+=len(documents)\n",
    "    document_count_per_year.append(count)\n",
    "            \n",
    "    \n",
    "df = pd.DataFrame(document_count_per_year, columns=['documents']) \n",
    "df.index = years\n",
    "pickle.dump(df, open(\"../stacked_vectors/uspto_document_count_by_year.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(2006, 2021):\n",
    "    year_filenames = []\n",
    "    for m in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "        year_filenames.append(str(y)+m)\n",
    "    series = uspto.loc[year_filenames].sum()\n",
    "    series = series.rename(y)\n",
    "    df = df.append(series)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df, open(\"../stacked_vectors/uspto_by_year.p\", \"wb\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multilingual</th>\n",
       "      <th>bottleneck</th>\n",
       "      <th>feature</th>\n",
       "      <th>improving</th>\n",
       "      <th>asr</th>\n",
       "      <th>performance</th>\n",
       "      <th>code</th>\n",
       "      <th>switched</th>\n",
       "      <th>speech</th>\n",
       "      <th>resourced</th>\n",
       "      <th>...</th>\n",
       "      <th>interface definition language idl</th>\n",
       "      <th>mu double</th>\n",
       "      <th>spl mu double</th>\n",
       "      <th>block motion compensation</th>\n",
       "      <th>ttcn</th>\n",
       "      <th>layout tool</th>\n",
       "      <th>1999a</th>\n",
       "      <th>verification reactive system</th>\n",
       "      <th>embedded bit stream</th>\n",
       "      <th>event list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>60</td>\n",
       "      <td>1319</td>\n",
       "      <td>66490</td>\n",
       "      <td>13353</td>\n",
       "      <td>143</td>\n",
       "      <td>36846</td>\n",
       "      <td>29087</td>\n",
       "      <td>9519</td>\n",
       "      <td>2949</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>66</td>\n",
       "      <td>1283</td>\n",
       "      <td>71218</td>\n",
       "      <td>13970</td>\n",
       "      <td>197</td>\n",
       "      <td>38797</td>\n",
       "      <td>32875</td>\n",
       "      <td>9569</td>\n",
       "      <td>3273</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>64</td>\n",
       "      <td>1380</td>\n",
       "      <td>74139</td>\n",
       "      <td>14101</td>\n",
       "      <td>151</td>\n",
       "      <td>39308</td>\n",
       "      <td>34879</td>\n",
       "      <td>9626</td>\n",
       "      <td>3215</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>35</td>\n",
       "      <td>1183</td>\n",
       "      <td>69315</td>\n",
       "      <td>13584</td>\n",
       "      <td>150</td>\n",
       "      <td>36562</td>\n",
       "      <td>31853</td>\n",
       "      <td>9367</td>\n",
       "      <td>2784</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>58</td>\n",
       "      <td>1342</td>\n",
       "      <td>75721</td>\n",
       "      <td>14570</td>\n",
       "      <td>144</td>\n",
       "      <td>39957</td>\n",
       "      <td>34498</td>\n",
       "      <td>9643</td>\n",
       "      <td>2987</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>61</td>\n",
       "      <td>1430</td>\n",
       "      <td>86729</td>\n",
       "      <td>16795</td>\n",
       "      <td>148</td>\n",
       "      <td>45513</td>\n",
       "      <td>39234</td>\n",
       "      <td>10498</td>\n",
       "      <td>3666</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>63</td>\n",
       "      <td>1749</td>\n",
       "      <td>103367</td>\n",
       "      <td>18927</td>\n",
       "      <td>265</td>\n",
       "      <td>52327</td>\n",
       "      <td>48687</td>\n",
       "      <td>11908</td>\n",
       "      <td>4682</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>68</td>\n",
       "      <td>1875</td>\n",
       "      <td>115690</td>\n",
       "      <td>20987</td>\n",
       "      <td>312</td>\n",
       "      <td>57262</td>\n",
       "      <td>53011</td>\n",
       "      <td>12579</td>\n",
       "      <td>5199</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>67</td>\n",
       "      <td>1913</td>\n",
       "      <td>120178</td>\n",
       "      <td>22000</td>\n",
       "      <td>316</td>\n",
       "      <td>58763</td>\n",
       "      <td>54035</td>\n",
       "      <td>12731</td>\n",
       "      <td>4998</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>80</td>\n",
       "      <td>1945</td>\n",
       "      <td>124431</td>\n",
       "      <td>22448</td>\n",
       "      <td>302</td>\n",
       "      <td>60273</td>\n",
       "      <td>57093</td>\n",
       "      <td>12704</td>\n",
       "      <td>5044</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>77</td>\n",
       "      <td>1901</td>\n",
       "      <td>123612</td>\n",
       "      <td>22514</td>\n",
       "      <td>331</td>\n",
       "      <td>59554</td>\n",
       "      <td>56615</td>\n",
       "      <td>11933</td>\n",
       "      <td>5500</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>75</td>\n",
       "      <td>1860</td>\n",
       "      <td>116652</td>\n",
       "      <td>20950</td>\n",
       "      <td>400</td>\n",
       "      <td>55199</td>\n",
       "      <td>52154</td>\n",
       "      <td>10855</td>\n",
       "      <td>5469</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>47</td>\n",
       "      <td>1482</td>\n",
       "      <td>90173</td>\n",
       "      <td>16655</td>\n",
       "      <td>361</td>\n",
       "      <td>42132</td>\n",
       "      <td>39514</td>\n",
       "      <td>8082</td>\n",
       "      <td>4420</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>19</td>\n",
       "      <td>769</td>\n",
       "      <td>46887</td>\n",
       "      <td>8828</td>\n",
       "      <td>131</td>\n",
       "      <td>22263</td>\n",
       "      <td>21038</td>\n",
       "      <td>4458</td>\n",
       "      <td>2395</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>5754</td>\n",
       "      <td>1186</td>\n",
       "      <td>18</td>\n",
       "      <td>2734</td>\n",
       "      <td>2788</td>\n",
       "      <td>564</td>\n",
       "      <td>344</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 1021362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     multilingual bottleneck feature improving  asr performance   code  \\\n",
       "2006           60       1319   66490     13353  143       36846  29087   \n",
       "2007           66       1283   71218     13970  197       38797  32875   \n",
       "2008           64       1380   74139     14101  151       39308  34879   \n",
       "2009           35       1183   69315     13584  150       36562  31853   \n",
       "2010           58       1342   75721     14570  144       39957  34498   \n",
       "2011           61       1430   86729     16795  148       45513  39234   \n",
       "2012           63       1749  103367     18927  265       52327  48687   \n",
       "2013           68       1875  115690     20987  312       57262  53011   \n",
       "2014           67       1913  120178     22000  316       58763  54035   \n",
       "2015           80       1945  124431     22448  302       60273  57093   \n",
       "2016           77       1901  123612     22514  331       59554  56615   \n",
       "2017           75       1860  116652     20950  400       55199  52154   \n",
       "2018           47       1482   90173     16655  361       42132  39514   \n",
       "2019           19        769   46887      8828  131       22263  21038   \n",
       "2020            3        111    5754      1186   18        2734   2788   \n",
       "\n",
       "     switched speech resourced    ...     interface definition language idl  \\\n",
       "2006     9519   2949         5    ...                                    24   \n",
       "2007     9569   3273         5    ...                                    22   \n",
       "2008     9626   3215         6    ...                                    22   \n",
       "2009     9367   2784         9    ...                                     3   \n",
       "2010     9643   2987        11    ...                                     6   \n",
       "2011    10498   3666         7    ...                                    13   \n",
       "2012    11908   4682        14    ...                                    13   \n",
       "2013    12579   5199        20    ...                                    12   \n",
       "2014    12731   4998        26    ...                                    13   \n",
       "2015    12704   5044        28    ...                                     4   \n",
       "2016    11933   5500        54    ...                                     8   \n",
       "2017    10855   5469        45    ...                                     4   \n",
       "2018     8082   4420        21    ...                                     3   \n",
       "2019     4458   2395         8    ...                                     2   \n",
       "2020      564    344         2    ...                                     1   \n",
       "\n",
       "     mu double spl mu double block motion compensation ttcn layout tool 1999a  \\\n",
       "2006         0             0                        21    1          56     3   \n",
       "2007         0             0                        27    0          39     2   \n",
       "2008         0             0                        22    0          52     2   \n",
       "2009         0             0                        32    0          35     2   \n",
       "2010         0             0                        21    0          44     2   \n",
       "2011         0             0                        23    1          43     3   \n",
       "2012         0             0                        26    0          41     4   \n",
       "2013         0             0                        27    1          46     4   \n",
       "2014         0             0                        36    0          40     2   \n",
       "2015         0             0                        47    1          41     1   \n",
       "2016         0             0                        53    0          49     1   \n",
       "2017         0             0                        51    1          38     3   \n",
       "2018         0             0                        37    3          27     2   \n",
       "2019         0             0                        33    0          29     1   \n",
       "2020         0             0                        11    0           5     1   \n",
       "\n",
       "     verification reactive system embedded bit stream event list  \n",
       "2006                            0                   7         59  \n",
       "2007                            0                  16         62  \n",
       "2008                            0                   4         78  \n",
       "2009                            0                   4         55  \n",
       "2010                            0                   4         57  \n",
       "2011                            0                   5         53  \n",
       "2012                            0                   4         81  \n",
       "2013                            0                   4         79  \n",
       "2014                            0                   0         79  \n",
       "2015                            0                   5         81  \n",
       "2016                            0                  15         89  \n",
       "2017                            0                  15         90  \n",
       "2018                            0                   3         43  \n",
       "2019                            0                   2         38  \n",
       "2020                            0                   0          2  \n",
       "\n",
       "[15 rows x 1021362 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 3.6608619689941406\n",
      "2001 3.271656036376953\n",
      "2002 3.6599881649017334\n",
      "2003 4.528121709823608\n",
      "2004 4.262118577957153\n",
      "2005 4.344035863876343\n",
      "2006 4.5587427616119385\n",
      "2007 5.31236457824707\n",
      "2008 6.152634859085083\n",
      "2009 6.9369494915008545\n",
      "2010 6.028690576553345\n",
      "2011 5.961461544036865\n",
      "2012 5.977890968322754\n",
      "2013 5.619373083114624\n",
      "2014 6.165205240249634\n",
      "2015 6.9628005027771\n",
      "2016 7.047996997833252\n",
      "2017 6.690347909927368\n",
      "2018 7.752470254898071\n",
      "2019 7.742755651473999\n",
      "2020 7.8365442752838135\n"
     ]
    }
   ],
   "source": [
    "document_count_per_year = []\n",
    "funders = ['Direct_For_Computer_&_Info_Scie_&_Enginr', 'Directorate_For_Engineering', 'Direct_For_Mathematical_&_Physical_Scien']\n",
    "\n",
    "years = list(range(2000, 2021))\n",
    "vectors = []\n",
    "for year in years:\n",
    "    t0 = time.time()\n",
    "    documents = []\n",
    "    for funder in funders:\n",
    "        with open(\"../../Data/nsf_cleaned/\"+funder+'/'+str(year)+\".txt\", \"r\") as f:\n",
    "            docs = f.readlines()\n",
    "            documents += [d.strip() for d in docs] \n",
    "    \n",
    "    document_count_per_year.append(len(documents))\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             ngram_range=(1,4),\n",
    "                             vocabulary=vocabulary,\n",
    "                             stop_words=stop\n",
    "                            )\n",
    "    \n",
    "    vector = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    del documents\n",
    "    \n",
    "    vector[vector>1] = 1\n",
    "    summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "    \n",
    "    del vector\n",
    "    \n",
    "    vectors.append(summed)\n",
    "    \n",
    "    del summed\n",
    "    \n",
    "    print(year, time.time()-t0)\n",
    "    \n",
    "# Turn the vector into a pandas dataframe\n",
    "# This has to happen because list indexing is waaaaaay too slow\n",
    "df = pd.DataFrame(vectors, columns=vocabulary) \n",
    "df.index = years\n",
    "pickle.dump(df, open(\"../stacked_vectors/nsf.p\", \"wb\"))\n",
    "\n",
    "del vectors\n",
    "del df\n",
    "\n",
    "df = pd.DataFrame(document_count_per_year, columns=['documents']) \n",
    "df.index = years\n",
    "pickle.dump(df, open(\"../stacked_vectors/nsf_document_count.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result, open(\"../stacked_vectors/semantic_scholar.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
