{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic scholar data\n",
    "\n",
    "We downloaded a subset of data from semantic scholar from here:\n",
    "\n",
    "http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/download/\n",
    "\n",
    "\n",
    "http://s2-public-api.prod.s2.allenai.org/corpus/\n",
    "[new link]\n",
    "\n",
    "\n",
    "Using the command \n",
    "\n",
    "```aws s3 cp --no-sign-request --recursive s3://ai2-s2-research-public/open-corpus/2021-03-01/ destinationPath```\n",
    "\n",
    "Some notes on this:\n",
    "- The data is very large, with 5000 gz files\n",
    "- The data is so large that I can only download in batches, unzip in batches, process, resave the processed data, and then delete the old data. At this point I can download another batch.\n",
    "- \"sources\":[\"DBLP\"]\n",
    "\n",
    "aws s3 cp --no-sign-request --exclude \"*\" --include \"*-2*\" --exclude \"*-20*\" --exclude \"*-21*\" --recursive s3://ai2-s2-research-public/open-corpus/2021-03-01/ C:\\Users\\[me]\\Documents\\CDT\\Data\\semanticscholar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import boto3\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../tools\")\n",
    "\n",
    "import tools\n",
    "import cleaning\n",
    "import html\n",
    "\n",
    "def extract_data(document):\n",
    "    '''\n",
    "    Extract title, year and abstract from a semantic scholar line\n",
    "    Replace any newlines or carriage returns with spaces\n",
    "    '''\n",
    "    json_document = json.loads(document)\n",
    "    title = json_document['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "    abstract = json_document['paperAbstract'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "    sources = json_document['sources']\n",
    "    year = json_document['year']\n",
    "    if 'DBLP' in sources:\n",
    "        extracted_data = {\n",
    "            'title': title,\n",
    "            'paperAbstract': abstract\n",
    "        }\n",
    "\n",
    "    \n",
    "        return year, json.dumps(extracted_data)\n",
    "    \n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a document\n",
    "\n",
    "```\n",
    "{\n",
    "    \"id\":\"7bbfdcca4478ba11e511cc46546e5da9fc82fe19\",\n",
    "    \"title\":\"Enhancing the TORA protocol using network localization and selective node participation\",\n",
    "    \"paperAbstract\":\"The Temporally-Ordered Routing Algorithm (TORA) is a distributed routing protocol that is based on a family of link reversal algorithms. TORA is able to provide multiple loop-free routes to any destination using the route creation, maintenance and erasure functions. TORA performs well in networks with a small number of traffic connections but poorly in networks with a large number of traffic connections. This poor performance is due to the traffic congestion caused by excessive route maintenance. This traffic congestion is further aggravated by routing overhead produced by the large number of traffic connections. We propose two modifications to improve TORA using a network localization approach and selective node participation approach. The network localization approach initializes and maintains a localized portion of the entire network while the selective node participation approach selects a subset of nodes to participate as part of the network. Benchmarks against original TORA show that our TORA modifications results in an overall performance improvement in terms of packet delivery, routing overhead and packet latency.\",\n",
    "    \"authors\":[\n",
    "        {\"name\":\"Kwan Hui Lim\",\"ids\":[\"4089267\"]},\n",
    "        {\"name\":\"Amitava  Datta\",\"ids\":[\"1716678\"]}\n",
    "        ],\n",
    "    \"inCitations\":[\"07fb7dec071f73d10e0fc6b6e6c065e4297e9006\",\"48d2d86f355f1aca5d71e4a230980c3da8d9c5f6\",\"2b9e549f406c46a33f4c98d9c2b350388e96af74\",\"0e75fee74633d3c776602feadbd2dda9a059b950\",\"d1c86fb6d5ef8c9f97cec9865f9cb58734c9458f\",\"5287d1ab116cb8ce1353c274cba868038dc820fc\"],\n",
    "    \"outCitations\":[\"53937dd143269339fc35cba93d397f8fcff62d1b\",\"21e5ce796636e566642224d4737ee3e0eae07470\",\"1c7f78f506d3409f1efa137d51169d5d87fdd24a\",\"673892e326414dfd9e72853048b467d6e75d16d8\",\"9928bfab5ef374e42ab30f8222be9c460afef313\",\"a8c934aa4b9d2736f97421354c51d0f11bfeb63b\",\"cbf3f6628a039f1542324860c2b2363b19ab2619\",\"9548b1a9142a5297c63cd254901eb751af80acd3\",\"5f9d49753857692e342d094a2f417e8de37ad18c\",\"ca95be7fef2ba6e1a98bd1fdbb04dcf140b2ad33\",\"0f28d106e7dc6464166f83583ee496559daa8b7f\"],\n",
    "    \"year\":2012,\n",
    "    \"s2Url\":\"https://semanticscholar.org/paper/7bbfdcca4478ba11e511cc46546e5da9fc82fe19\",\n",
    "    \"sources\":[\"DBLP\"],\n",
    "    \"pdfUrls\":[\"https://doi.org/10.1109/PIMRC.2012.6362586\",\"http://staffhome.ecm.uwa.edu.au/~10449838/2012-PIMRC-toraPaper.pdf\"],\n",
    "    \"venue\":\"2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)\",\n",
    "    \"journalName\":\"2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)\",\n",
    "    \"journalVolume\":\"\",\n",
    "    \"journalPages\":\"1503-1508\",\n",
    "    \"doi\":\"10.1109/PIMRC.2012.6362586\",\n",
    "    \"doiUrl\":\"https://doi.org/10.1109/PIMRC.2012.6362586\",\n",
    "    \"pmid\":\"\",\n",
    "    \"fieldsOfStudy\":[\"Computer Science\"],\n",
    "    \"magId\":\"2132186477\",\n",
    "    \"s2PdfUrl\":\"\",\n",
    "    \"entities\":[]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important fields are\n",
    "- Title\n",
    "- paperAbstract\n",
    "- sources\n",
    "- year\n",
    "- fieldsOfStudy\n",
    "\n",
    "### Load file and translate the first line to json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Data/semanticscholar_sample/sample-S2-records\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":\"989e305765a01478d9a786987fb6f4fc379da91b\",\"title\":\"FRANCESCO GUERRA â€• NADIA ROBOTTI, Ettore Majorana. Aspects of his Scientific and Academic Activity. Pisa: Edizioni Scuola Normale Superiore, 2008. 243 pp., ISBN 978-88-7642-331-4.\",\"paperAbstract\":\"\",\"authors\":[{\"name\":\"Luisa  Bonolis\",\"ids\":[\"15983375\"]}],\"inCitations\":[],\"outCitations\":[],\"year\":2009,\"s2Url\":\"https://semanticscholar.org/paper/989e305765a01478d9a786987fb6f4fc379da91b\",\"sources\":[],\"pdfUrls\":[],\"venue\":\"\",\"journalName\":\"Nuncius-journal of The History of Science\",\"journalVolume\":\"24\",\"journalPages\":\"540-541\",\"doi\":\"10.1163/182539109X00912\",\"doiUrl\":\"https://doi.org/10.1163/182539109X00912\",\"pmid\":\"\",\"fieldsOfStudy\":[\"Philosophy\"],\"magId\":\"2090086602\",\"s2PdfUrl\":\"\",\"entities\":[]}\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_record = json.loads(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '376c7945fa36d821ab9cf40f9eb307528bca2f88',\n",
       " 'title': 'UMP lost and found mobile application',\n",
       " 'paperAbstract': 'Since generation, the issue of losing personal belongings is a common thing for all people. Anyone can lose their personal belongings. At Universiti Malaysia Pahang only, almost every week there will be a lot of cases of lost and found personal items such as wallets, matric cards, room keys and so on. Usually, the information regarding the lost and found item was spread by the student himself via the UMP portal in the announcement board and the WhatsApp media. This method is not very efficient because not all students at UMP will receive information about the lost and found items. Hence, UMP Lost and Found Mobile Application is developed to centralized the information regarding all the lost and found items in a mobile application. This app can help students find lost items or track owners for items they have found. Any lost or found items around UMP will be reported and recorded in this mobile app by the students, with all records relating to lost or found items will be easily searched by students to search for their missing items.',\n",
       " 'authors': [{'name': 'Muhammad  Syafiq', 'ids': ['152797367']},\n",
       "  {'name': 'Muhammad Razman Robert', 'ids': ['1576094093']}],\n",
       " 'inCitations': [],\n",
       " 'outCitations': [],\n",
       " 'year': 2019,\n",
       " 's2Url': 'https://semanticscholar.org/paper/376c7945fa36d821ab9cf40f9eb307528bca2f88',\n",
       " 'sources': [],\n",
       " 'pdfUrls': [],\n",
       " 'venue': '',\n",
       " 'journalName': '',\n",
       " 'journalVolume': '',\n",
       " 'journalPages': '',\n",
       " 'doi': '',\n",
       " 'doiUrl': '',\n",
       " 'pmid': '',\n",
       " 'fieldsOfStudy': ['Computer Science'],\n",
       " 'magId': '3009022056',\n",
       " 's2PdfUrl': '',\n",
       " 'entities': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_record['sources']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "- It is easy to read data in the json format and extract information\n",
    "- Not every record seems to have an associated source.\n",
    "- Records can have 0+ \"fields of study\". However it is not clear how these fields of study are assigned and the documentation is opaque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_documents = defaultdict(list)\n",
    "f = open(\"../../Data/semanticscholar_sample/sample-S2-records\", \"r\", encoding=\"utf8\")\n",
    "for i, line in enumerate(f):\n",
    "    year, extracted_data = extract_data(line)\n",
    "    if year is not None:\n",
    "        dblp_documents[year].append(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {2012: ['{\"title\": \"Enhancing the TORA protocol using network localization and selective node participation\", \"paperAbstract\": \"The Temporally-Ordered Routing Algorithm (TORA) is a distributed routing protocol that is based on a family of link reversal algorithms. TORA is able to provide multiple loop-free routes to any destination using the route creation, maintenance and erasure functions. TORA performs well in networks with a small number of traffic connections but poorly in networks with a large number of traffic connections. This poor performance is due to the traffic congestion caused by excessive route maintenance. This traffic congestion is further aggravated by routing overhead produced by the large number of traffic connections. We propose two modifications to improve TORA using a network localization approach and selective node participation approach. The network localization approach initializes and maintains a localized portion of the entire network while the selective node participation approach selects a subset of nodes to participate as part of the network. Benchmarks against original TORA show that our TORA modifications results in an overall performance improvement in terms of packet delivery, routing overhead and packet latency.\"}'],\n",
       "             2007: ['{\"title\": \"Dynamic batch processing in workflows: Model and implementation\", \"paperAbstract\": \"Dynamic batch processing commonly exists in production or business processes. Traditional workflow management systems (WfMSs) do not support dynamic batch processing. This paper applies dynamic batch processing to WfMSs. The proposed mechanism consists of a dynamic batch processing scheduling model as well as the design and implementation. Our experiment shows that the proposed dynamic batch processing mechanism is useful for and convenient to business processes.\"}'],\n",
       "             2020: ['{\"title\": \"Virtual Development and Validation of a Function for an Automated Lateral Control using Artificial Neural Networks and Genetic Algorithms\", \"paperAbstract\": \"In this paper the development of a function for an automated lateral guidance by means of a consistently model-based and verification-oriented methodology of development will be presented. With a virtual test bench being used, the development process starts out with modelling the vehicle dynamics, the sensor system, and the environment. It is followed by a detailed layout of a self-learning driving function for the vehicle to stay in its lane on any single-lane road using artificial neural networks (ANN) and genetic algorithms (GA). The process is completed by a systematic analysis and validation of the thus developed driving function for lateral guidance.\"}'],\n",
       "             2017: ['{\"title\": \"Auditory Verbal Experience and Agency in Waking, Sleep Onset, REM, and Non-REM Sleep\", \"paperAbstract\": \"We present one of the first quantitative studies on auditory verbal experiences (\\\\\"hearing voices\\\\\") and auditory verbal agency (inner speech, and specifically \\\\\"talking to (imaginary) voices or characters\\\\\") in healthy participants across states of consciousness. Tools of quantitative linguistic analysis were used to measure participants\\' implicit knowledge of auditory verbal experiences (VE) and auditory verbal agencies (VA), displayed in mentation reports from four different states. Analysis was conducted on a total of 569 mentation reports from rapid eye movement (REM) sleep, non-REM sleep, sleep onset, and waking. Physiology was controlled with the nightcap sleep-wake mentation monitoring system. Sleep-onset hallucinations, traditionally at the focus of scientific attention on auditory verbal hallucinations, showed the lowest degree of VE and VA, whereas REM sleep showed the highest degrees. Degrees of different linguistic-pragmatic aspects of VE and VA likewise depend on the physiological states. The quantity and pragmatics of VE and VA are a function of the physiologically distinct state of consciousness in which they are conceived.\"}',\n",
       "              '{\"title\": \"Systematic Pattern Approach for Safety and Security Co-engineering in the Automotive Domain\", \"paperAbstract\": \"Future automotive systems will exhibit increased levels of automation as well as ever tighter integration with other vehicles, traffic infrastructure, and cloud services. From safety perspective, this can be perceived as boon or bane - it greatly increases complexity and uncertainty, but at the same time opens up new opportunities for realizing innovative safety functions. Moreover, cybersecurity becomes important as additional concern because attacks are now much more likely and severe. Unfortunately, there is lack of experience with security concerns in context of safety engineering in general and in automotive safety departments in particular. To remediate this problem, we propose a systematic pattern-based approach that interlinks safety and security patterns and provides guidance with respect to selection and combination of both types of patterns in context of system engineering. The application of a combined safety and security pattern engineering workflow is shown and demonstrated by an automotive use case scenario.\"}'],\n",
       "             2014: ['{\"title\": \"A Classifier Based on a Decision Tree with Verifying Cuts\", \"paperAbstract\": \"This article introduces a new method of a decision tree construction. Such construction is performed using additional cuts applied for a verificatio n of the cuts\\' quality in tree nodes during the classification of objects. The presented approach allow s us to exploit the additional knowledge represented in the attributes which could be eliminated using greedy methods. The paper includes the results of experiments performed on data sets from a biomedical database and machine learning repositories. In order to evaluate the presented method, we compared its performance with the classification results of a local discretization decision t ree, well known from literature. Our new method outperforms the existing method, which is also confir med by statistical tests.\"}']})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in dblp_documents.keys():\n",
    "    with open(\"../../Data/semanticscholar_sample/year_sorted_data/\"+str(year)+\".txt\", \"a\") as f:\n",
    "        for document in dblp_documents[year]:\n",
    "            f.write(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try on real data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_documents = defaultdict(list)\n",
    "f = open(\"../../Data/semanticscholar/s2-corpus-000\", \"r\", encoding=\"utf8\")\n",
    "for i, line in enumerate(f):\n",
    "    year, extracted_data = extract_data(line)\n",
    "    if year is not None:\n",
    "        dblp_documents[year].append(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1974 2\n",
      "1975 2\n",
      "1976 2\n",
      "1982 1\n",
      "1983 2\n",
      "1984 1\n",
      "1985 3\n",
      "1986 1\n",
      "1987 2\n",
      "1988 4\n",
      "1989 1\n",
      "1990 6\n",
      "1991 3\n",
      "1992 4\n",
      "1993 6\n",
      "1994 7\n",
      "1995 10\n",
      "1996 10\n",
      "1997 12\n",
      "1998 8\n",
      "1999 10\n",
      "2000 18\n",
      "2001 9\n",
      "2002 16\n",
      "2003 14\n",
      "2004 19\n",
      "2005 33\n",
      "2006 26\n",
      "2007 27\n",
      "2008 38\n",
      "2009 37\n",
      "2010 30\n",
      "2011 32\n",
      "2012 40\n",
      "2013 51\n",
      "2014 36\n",
      "2015 49\n",
      "2016 50\n",
      "2017 49\n",
      "2018 60\n",
      "2019 56\n",
      "2020 62\n",
      "2021 4\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(dblp_documents.keys()):\n",
    "    print(key, len(dblp_documents[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a set of files in an interval and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 897\n",
      "601 899\n",
      "602 884\n",
      "603 939\n",
      "604 873\n",
      "605 838\n",
      "606 877\n",
      "607 882\n",
      "608 900\n",
      "609 916\n",
      "610 878\n",
      "611 831\n",
      "612 864\n",
      "613 871\n",
      "614 865\n",
      "615 880\n",
      "616 837\n",
      "617 912\n",
      "618 889\n",
      "619 927\n",
      "620 863\n",
      "621 886\n",
      "622 861\n",
      "623 918\n",
      "624 865\n",
      "625 907\n",
      "626 870\n",
      "627 851\n",
      "628 871\n",
      "629 901\n",
      "630 849\n",
      "631 911\n",
      "632 926\n",
      "633 870\n",
      "634 928\n",
      "635 871\n",
      "636 893\n",
      "637 936\n",
      "638 912\n",
      "639 886\n",
      "640 928\n",
      "641 838\n",
      "642 914\n",
      "643 913\n",
      "644 916\n",
      "645 871\n",
      "646 905\n",
      "647 927\n",
      "648 842\n",
      "649 914\n",
      "650 919\n",
      "651 883\n",
      "652 845\n",
      "653 920\n",
      "654 874\n",
      "655 889\n",
      "656 897\n",
      "657 846\n",
      "658 914\n",
      "659 887\n",
      "660 895\n",
      "661 950\n",
      "662 875\n",
      "663 873\n",
      "664 858\n",
      "665 887\n",
      "666 905\n",
      "667 897\n",
      "668 891\n",
      "669 858\n",
      "670 863\n",
      "671 897\n",
      "672 856\n",
      "673 880\n",
      "674 885\n",
      "675 896\n",
      "676 936\n",
      "677 855\n",
      "678 876\n",
      "679 866\n",
      "680 871\n",
      "681 853\n",
      "682 912\n",
      "683 891\n",
      "684 859\n",
      "685 899\n",
      "686 908\n",
      "687 895\n",
      "688 881\n",
      "689 887\n",
      "690 909\n",
      "691 864\n",
      "692 916\n",
      "693 860\n",
      "694 844\n",
      "695 895\n",
      "696 876\n",
      "697 894\n",
      "698 890\n",
      "699 859\n",
      "700 889\n",
      "701 858\n",
      "702 895\n",
      "703 886\n",
      "704 849\n",
      "705 866\n",
      "706 892\n",
      "707 870\n",
      "708 854\n",
      "709 902\n",
      "710 845\n",
      "711 866\n",
      "712 880\n",
      "713 872\n",
      "714 912\n",
      "715 895\n",
      "716 909\n",
      "717 896\n",
      "718 848\n",
      "719 903\n",
      "720 917\n",
      "721 896\n",
      "722 898\n",
      "723 890\n",
      "724 945\n",
      "725 945\n",
      "726 896\n",
      "727 897\n",
      "728 913\n",
      "729 889\n",
      "730 861\n",
      "731 871\n",
      "732 903\n",
      "733 890\n",
      "734 905\n",
      "735 897\n",
      "736 894\n",
      "737 869\n",
      "738 851\n",
      "739 860\n",
      "740 855\n",
      "741 935\n",
      "742 894\n",
      "743 885\n",
      "744 898\n",
      "745 855\n",
      "746 995\n",
      "747 904\n",
      "748 900\n",
      "749 896\n",
      "750 866\n",
      "751 864\n",
      "752 906\n",
      "753 876\n",
      "754 893\n",
      "755 850\n",
      "756 851\n",
      "757 896\n",
      "758 869\n",
      "759 899\n",
      "760 923\n",
      "761 876\n",
      "762 861\n",
      "763 877\n",
      "764 912\n",
      "765 876\n",
      "766 904\n",
      "767 927\n",
      "768 904\n",
      "769 932\n",
      "770 886\n",
      "771 894\n",
      "772 885\n",
      "773 877\n",
      "774 853\n",
      "775 937\n",
      "776 858\n",
      "777 864\n",
      "778 844\n",
      "779 903\n",
      "780 847\n",
      "781 895\n",
      "782 866\n",
      "783 894\n",
      "784 903\n",
      "785 903\n",
      "786 860\n",
      "787 964\n",
      "788 873\n",
      "789 921\n",
      "790 810\n",
      "791 866\n",
      "792 885\n",
      "793 879\n",
      "794 895\n",
      "795 878\n",
      "796 851\n",
      "797 903\n",
      "798 936\n",
      "799 885\n",
      "800 899\n",
      "801 880\n",
      "802 855\n",
      "803 886\n",
      "804 871\n",
      "805 891\n",
      "806 878\n",
      "807 917\n",
      "808 902\n",
      "809 885\n",
      "810 910\n",
      "811 881\n",
      "812 857\n",
      "813 917\n",
      "814 939\n",
      "815 882\n",
      "816 843\n",
      "817 913\n",
      "818 883\n",
      "819 906\n",
      "820 905\n",
      "821 879\n",
      "822 870\n",
      "823 934\n",
      "824 920\n",
      "825 875\n",
      "826 867\n",
      "827 890\n",
      "828 892\n",
      "829 925\n",
      "830 871\n",
      "831 884\n",
      "832 902\n",
      "833 859\n",
      "834 946\n",
      "835 925\n",
      "836 868\n",
      "837 931\n",
      "838 876\n",
      "839 919\n",
      "840 867\n",
      "841 899\n",
      "842 896\n",
      "843 898\n",
      "844 864\n",
      "845 891\n",
      "846 856\n",
      "847 876\n",
      "848 856\n",
      "849 852\n",
      "850 902\n",
      "851 932\n",
      "852 889\n",
      "853 890\n",
      "854 868\n",
      "855 902\n",
      "856 901\n",
      "857 852\n",
      "858 876\n",
      "859 879\n",
      "860 901\n",
      "861 929\n",
      "862 882\n",
      "863 869\n",
      "864 906\n",
      "865 895\n",
      "866 844\n",
      "867 905\n",
      "868 877\n",
      "869 922\n",
      "870 900\n",
      "871 885\n",
      "872 890\n",
      "873 891\n",
      "874 890\n",
      "875 957\n",
      "876 909\n",
      "877 812\n",
      "878 859\n",
      "879 888\n",
      "880 921\n",
      "881 913\n",
      "882 901\n",
      "883 895\n",
      "884 891\n",
      "885 823\n",
      "886 868\n",
      "887 872\n",
      "888 881\n",
      "889 879\n",
      "890 921\n",
      "891 894\n",
      "892 859\n",
      "893 893\n",
      "894 820\n",
      "895 863\n",
      "896 943\n",
      "897 892\n",
      "898 892\n",
      "899 934\n",
      "900 868\n",
      "901 890\n",
      "902 880\n",
      "903 924\n",
      "904 889\n",
      "905 853\n",
      "906 791\n",
      "907 936\n",
      "908 908\n",
      "909 934\n",
      "910 847\n",
      "911 917\n",
      "912 900\n",
      "913 835\n",
      "914 934\n",
      "915 886\n",
      "916 931\n",
      "917 876\n",
      "918 886\n",
      "919 854\n",
      "920 882\n",
      "921 868\n",
      "922 870\n",
      "923 906\n",
      "924 946\n",
      "925 948\n",
      "926 838\n",
      "927 938\n",
      "928 916\n",
      "929 886\n",
      "930 891\n",
      "931 891\n",
      "932 896\n",
      "933 873\n",
      "934 936\n",
      "935 856\n",
      "936 919\n",
      "937 889\n",
      "938 869\n",
      "939 898\n",
      "940 943\n",
      "941 875\n",
      "942 896\n",
      "943 898\n",
      "944 920\n",
      "945 888\n",
      "946 861\n",
      "947 866\n",
      "948 925\n",
      "949 858\n",
      "950 869\n",
      "951 888\n",
      "952 857\n",
      "953 893\n",
      "954 830\n",
      "955 844\n",
      "956 891\n",
      "957 913\n",
      "958 798\n",
      "959 884\n",
      "960 851\n",
      "961 885\n",
      "962 915\n",
      "963 874\n",
      "964 903\n",
      "965 892\n",
      "966 884\n",
      "967 898\n",
      "968 873\n",
      "969 866\n",
      "970 944\n",
      "971 909\n",
      "972 897\n",
      "973 882\n",
      "974 863\n",
      "975 910\n",
      "976 872\n",
      "977 868\n",
      "978 848\n",
      "979 907\n",
      "980 844\n",
      "981 873\n",
      "982 922\n",
      "983 909\n",
      "984 858\n",
      "985 895\n",
      "986 865\n",
      "987 834\n",
      "988 858\n",
      "989 852\n",
      "990 869\n",
      "991 869\n",
      "992 908\n",
      "993 923\n",
      "994 917\n",
      "995 925\n",
      "996 890\n",
      "997 922\n",
      "998 853\n",
      "999 907\n"
     ]
    }
   ],
   "source": [
    "for filenumber in np.arange(600, 1000):\n",
    "    if len(str(filenumber))==1:\n",
    "        filenumber = '00'+str(filenumber)\n",
    "    elif len(str(filenumber))==2:\n",
    "        filenumber = '0'+str(filenumber)\n",
    "    else:\n",
    "        filenumber = str(filenumber)\n",
    "\n",
    "    dblp_documents = defaultdict(list)\n",
    "    f = open(\"../../Data/semanticscholar/s2-corpus-\"+filenumber, \"r\", encoding=\"utf8\")\n",
    "    for i, line in enumerate(f):\n",
    "        year, extracted_data = extract_data(line)\n",
    "        if year is not None:\n",
    "            dblp_documents[year].append(extracted_data)\n",
    "            \n",
    "    for year in dblp_documents.keys():\n",
    "        with open(\"../../Data/semantic_scholar_filtered/\"+str(year)+\".txt\", \"a\") as f:\n",
    "            for document in dblp_documents[year]:\n",
    "                f.write(document + '\\n')\n",
    "    \n",
    "    print(filenumber, sum([len(dblp_documents[key]) for key in dblp_documents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "\n",
    "At this point, we try out a cleaning pipeline. What data are we interested in?\n",
    "\n",
    "- title field must not be empty\n",
    "- abstract field must be greater than 50 characters in length\n",
    "- we need to be able to deal with unicode characters\n",
    "- lemmatise data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Data/semantic_scholar_filtered/1980.txt\", \"r\", encoding=\"utf8\")\n",
    "lines = []\n",
    "for i, line in enumerate(f):\n",
    "    lines.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract Let 0⩽ x 1 ⩽ x 2 ⩽ x 3 ⩽· be a sequence of real numbers, lim x i =+∞. We prove that there exists a sequence P ={ z 1 , z 2 , z 3 ,·} in E 2 such that |; z i | = x i and every straight line of E 2 comes arbitrarily near to P , if and only if Σ 1 x i =+∞ . Analogous results are valid for the case of higher dimensions.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove e.g. \\xa0 characters from string\n",
    "text = lines[892]['paperAbstract']\n",
    "text = unicodedata.normalize('NFKC', text)\n",
    "text = text.replace('\\n',' ')\n",
    "text = text.replace('\\r',' ')\n",
    "text = text.replace('\\t',' ')\n",
    "text = re.sub(r'&#?[a-z0-9]+;', ' ', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning pipeline\n",
    "\n",
    "- Remove special characters\n",
    "- Remove punctuation\n",
    "- Remove stopwords\n",
    "- Set everything to lowercase\n",
    "- lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasonable(title, abstract):\n",
    "    '''\n",
    "    Check that there is a title and that the abstract is greater than 50 characters in length\n",
    "    '''\n",
    "    if type(title) == float:\n",
    "        return False\n",
    "    if type(abstract) == float:\n",
    "        return False\n",
    "    if title == '':\n",
    "        return False\n",
    "    if len(abstract) < 50:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def normalise_acronymns(text):\n",
    "    '''\n",
    "    Remove the periods in acronyms. \n",
    "    Adapted from the method found at https://stackoverflow.com/a/40197005 \n",
    "    '''\n",
    "\n",
    "    # deal with single letters before sentence boundaries\n",
    "    text = re.sub(r'\\s([A-Z, a-z])\\.\\s', r' \\1..  ', text)\n",
    "    return re.sub(r'(?<!\\w)([A-Z, a-z])\\.', r'\\1', text)\n",
    "\n",
    "def normalise_decimals(text):\n",
    "    '''\n",
    "    Remove the periods in decimal numbers and replace with POINT\n",
    "    '''\n",
    "    return re.sub(r'([0-9])\\.([0-9])', r'\\1POINT\\2', text)\n",
    "\n",
    "def normalise_dashes(text):\n",
    "    '''\n",
    "    In cases where there is a dash connecting one or two letters to a longer word, preserve dashes\n",
    "    '''\n",
    "    # When it occurs at the start of text...\n",
    "    text = re.sub(r'(^[0-9a-zA-Z]{1,2})-([0-9a-zA-Z])', r'\\1DASH\\2', text)\n",
    "    \n",
    "    # When it occurs in the middle of text...\n",
    "    text = re.sub(r'([\\s\\W][0-9a-zA-Z]{1,2})-([0-9a-zA-Z])', r'\\1DASH\\2', text)\n",
    "    text = re.sub(r'([0-9a-zA-Z])-([0-9a-zA-Z]{1,2}[\\s\\W])', r'\\1DASH\\2', text)\n",
    "    \n",
    "    # When it occurs at the end of text\n",
    "    text = re.sub(r'([0-9a-zA-Z])-([0-9a-zA-Z]{1,2}$)', r'\\1DASH\\2', text)\n",
    "    return text\n",
    "    \n",
    "def clean(text, wnl, tokeniser):\n",
    "    # Remove special characters\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'&#?[0-9a-zA-Z]+;', ' ', text)\n",
    "    \n",
    "    # Remove line breaks\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\r',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    \n",
    "    # Preserve acronyms\n",
    "    text = normalise_acronymns(text)\n",
    "    # Preserve decimal points\n",
    "    text = normalise_decimals(text)\n",
    "    # Normalise dashes\n",
    "    text = normalise_dashes(text)\n",
    "    \n",
    "    # Lemmatise word by word\n",
    "    lemmas = []\n",
    "    for word in tokeniser(text):\n",
    "        lemmas.append(wnl.lemmatize(word))\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Data/semantic_scholar_filtered/1980.txt\", \"r\", encoding=\"utf8\")\n",
    "lines = []\n",
    "for i, line in enumerate(f):\n",
    "    lines.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some experiments in discrete utterance recognition This paper is concerned with the following three aspects of the discrete utterance recognition problem: utterance normalization, dynamic programming algorithm implementation, and boundary error effects. Performance sensitivity as a function of each aspect of the problem is comparatively studied utilizing several available alternatives and significant conclusions are drawn regarding each of them. The concept of proportional normalizing is introduced as an effective method of handling the utterance normalization problem. A database consisting of the utterances of the alpha-digit vocabulary produced by several male and female speakers is used to conduct all the experiments.\n",
      "--------------------------------------------------\n",
      "some experiment in discrete utterance recognition this paper is concerned with the following three aspect of the discrete utterance recognition problem utterance normalization dynamic programming algorithm implementation and boundary error effect performance sensitivity a function of each aspect of the problem is comparatively studied utilizing several available alternative and significant conclusion are drawn regarding each of them the concept of proportional normalizing is introduced a an effective method of handling the utterance normalization problem database consisting of the utterance of the alpha digit vocabulary produced by several male and female speaker is used to conduct all the experiment\n",
      "==================================================\n",
      "Online-Anwendungen im Rechnungswesen beim Großgversandhaus QUELLE Online-Anwendungen beim Grossversandhaus Quelle haben Geschichte. Bereits vor 25 Jahren, als weder von Online-Anwendungen die Rede noch der Begriff ‘Realtime’ erfunden war, gab Quelle die erste kommerzielle Realtime-Anlage bei Standard-Elektrik Lorenz in Auftrag. Im Jahr 1957 wurde die — damals hausintern „Elektronik“ genannte — Anlage in Betrieb genommen. Sie war der erste grose kommerzielle Computer auf der Welt, der nicht mit Elektronen-Rohren, sondern Transistoren arbeitete. Er bediente uber 100 Ein-/Ausgabeplatze und bewaltigte uber 10 Jahre lang die Bestandsfuhrung des Versandhauses.\n",
      "--------------------------------------------------\n",
      "online anwendungen im rechnungswesen beim grogversandhaus quelle online anwendungen beim grossversandhaus quelle haben geschichte bereits vor 25 jahren al weder von online anwendungen die rede noch der begriff realtime erfunden war gab quelle die erste kommerzielle realtime anlage bei standard elektrik lorenz in auftrag im jahr 1957 wurde die damals hausintern elektronik genannte anlage in betrieb genommen sie war der erste grose kommerzielle computer auf der welt der nicht mit elektronen rohren sondern transistoren arbeitete er bediente uber 100 ein ausgabeplatze und bewaltigte uber 10 jahre lang die bestandsfuhrung de versandhauses\n",
      "==================================================\n",
      "Buchhaltung im Dialog - eine Voraussetzung für mehr Wirtschaftlichkeit und Aktualität im Rechnungswesen Die Expansion unserer Wirtschaftswelt fordert immer mehr vervielfaltigte Leistungen auf der Basis immer knapper werdender Verhaltnisse.\n",
      "--------------------------------------------------\n",
      "buchhaltung im dialog eine voraussetzung fur mehr wirtschaftlichkeit und aktualitat im rechnungswesen die expansion unserer wirtschaftswelt fordert immer mehr vervielfaltigte leistungen auf der basis immer knapper werdender verhaltnisse\n",
      "==================================================\n",
      "Benefit-Cost Methodology for Evaluating Peak-Load Pricing System Implementation for Electricity Rates Many utilities are searching for ways to cut down their costs and to implement innovative rate designs. Economic analysis suggests that marginal cost pricing is necessary for efficient allocation. Specifically, peak-load pricing (PLP) for the use of electricity is widely recommended. However, such shifts are not without costs. In this paper the use of a cost-benefit framework to evaluate change to a rate structure based upon PLP is suggested. The use of the framework with some hypothetical examples is demonstrated.\n",
      "--------------------------------------------------\n",
      "benefit cost methodology for evaluating peak load pricing system implementation for electricity rate many utility are searching for way to cut down their cost and to implement innovative rate design economic analysis suggests that marginal cost pricing is necessary for efficient allocation specifically peak load pricing plp for the use of electricity is widely recommended however such shift are not without cost in this paper the use of cost benefit framework to evaluate change to rate structure based upon plp is suggested the use of the framework with some hypothetical example is demonstrated\n",
      "==================================================\n",
      "A backend machine architecture for information retrieval A textual database can be loosely defined to be a group of related documents, each containing an essentially unstructured string of characters and symbols, which describe some information in English or any other high-level natural language on a specific subject matter by use of a set of words, phrases and sentences that depend very much on the subject matter and the intended use of the document. Such databases cover a wide range of applications, viz. libraries, newspapers, medical diagnostics, abstracts of papers and dissertations, legal case reports, military and intelligence reports, etc. With the advent of highdensity memory technology and the availability of computerised typesetting and machine reading technology, an explosion in the growth of such databases for a variety of applications may be anticipated. This chapter is a preliminary study on an ongoing research effort devoted to the development of backend machine architecture for large textual databases or information retrieval systems. Search and retrieval operations on textual databases are complicated by the very nature of information that they contain. The text databases show a wide variation in formatting, and have a large number of oddities, redundancies, non-informational words and context-dependent as well as spelling ambiguities; the range of potential query is unrestricted. There is no data model that is applicable to develop a structured approach to search and retrieval, as is the case for conventional databases (viz. relational or hierarchical). Conventional machine architectures and software systems perform search and retrieval operations on such databases using a combination of inversion of text to produce an inverted list and sequential scan on the secondary storage media. These systems are inherently slow, because the machines do not have build-in hardware to do high-speed pattern matching, searching, sorting or retrieval operations. Furthermore, the phenomenon of a 'yon Neuman bottleneck' between the CPU and the main memory and the data transportation problem over a bandwidth-limited channel which uses complicated navigational procedures to locate data on serial-access bulk storage add to this slow performance and inefficiency. Furthermore, the inverted file system may add as much as 300 per cent storage overhead and needs rather time-\n",
      "--------------------------------------------------\n",
      "backend machine architecture for information retrieval textual database can be loosely defined to be group of related document each containing an essentially unstructured string of character and symbol which describe some information in english or any other high level natural language on specific subject matter by use of set of word phrase and sentence that depend very much on the subject matter and the intended use of the document such database cover wide range of application viz library newspaper medical diagnostics abstract of paper and dissertation legal case report military and intelligence report etc with the advent of highdensity memory technology and the availability of computerised typesetting and machine reading technology an explosion in the growth of such database for variety of application may be anticipated this chapter is preliminary study on an ongoing research effort devoted to the development of backend machine architecture for large textual database or information retrieval system search and retrieval operation on textual database are complicated by the very nature of information that they contain the text database show wide variation in formatting and have large number of oddity redundancy non informational word and context dependent a well a spelling ambiguity the range of potential query is unrestricted there is no data model that is applicable to develop structured approach to search and retrieval a is the case for conventional database viz relational or hierarchical conventional machine architecture and software system perform search and retrieval operation on such database using combination of inversion of text to produce an inverted list and sequential scan on the secondary storage medium these system are inherently slow because the machine do not have builddashin hardware to do high speed pattern matching searching sorting or retrieval operation furthermore the phenomenon of yon neuman bottleneck between the cpu and the main memory and the data transportation problem over bandwidth limited channel which us complicated navigational procedure to locate data on serial access bulk storage add to this slow performance and inefficiency furthermore the inverted file system may add a much a 300 per cent storage overhead and need rather time\n",
      "==================================================\n",
      "Map-guided interpretation of remotely-sensed imagery Aerial and satellite imagery provide an economical means of gathering large amounts of data on the earth's resources and environment. However, except in the area of survey tasks such as crop inventories and land use that can be performed with multispectral analysis, there are few economically feasible techniques for automatically extracting the useful information from such imagery.\n",
      "--------------------------------------------------\n",
      "map guided interpretation of remotely sensed imagery aerial and satellite imagery provide an economical mean of gathering large amount of data on the earth resource and environment however except in the area of survey task such a crop inventory and land use that can be performed with multispectral analysis there are few economically feasible technique for automatically extracting the useful information from such imagery\n",
      "==================================================\n",
      "Abstracts of selected journal articles Minicomputer-operated information retrieval (IR) systems are capable of employing relatively advanced methods, some of which are comparable with those employed in main-frame systems. Many of these systems operate on dedicated machines and can therefore provide very rapid access to information, while remaining under the direct control of an information department. One system has now given over three years of satisfactory operation: this is MORPHS Minicomputer Operated Retrieval (Partially Heuristic) System. This system incorporates a number of linguistic features including the ability to find roots of words through affix stripping. Synonyms and compound words can also be handled and several search strategies (including SDI) are available. The latter have been developed considerably since the inception of the system. Consideration is given to the automation of the indexing process which is currently restricted to material for SDI.\n",
      "--------------------------------------------------\n",
      "abstract of selected journal article minicomputer operated information retrieval ir system are capable of employing relatively advanced method some of which are comparable with those employed in main frame system many of these system operate on dedicated machine and can therefore provide very rapid access to information while remaining under the direct control of an information department one system ha now given over three year of satisfactory operation this is morphs minicomputer operated retrieval partially heuristic system this system incorporates number of linguistic feature including the ability to find root of word through affix stripping synonym and compound word can also be handled and several search strategy including sdi are available the latter have been developed considerably since the inception of the system consideration is given to the automation of the indexing process which is currently restricted to material for sdi\n",
      "==================================================\n",
      "The Publishing of scholarly monographs Monographs are conventionally books which report specialized research work to a restricted readership. The publishing of such books requires special publishing skills of both an editorial and financial type. Many scholarly publishers make great use of academic advisers and scouts who seek out research workers and can actually bring about the writing of new books. Scholars and publishers exist in a symbiotic relationship which contains areas of potential mutual benefit but also potential conflict. Good publishers are greatly appreciated by authors who themselves become a part of the communication network.\n",
      "--------------------------------------------------\n",
      "the publishing of scholarly monograph monograph are conventionally book which report specialized research work to restricted readership the publishing of such book requires special publishing skill of both an editorial and financial type many scholarly publisher make great use of academic adviser and scout who seek out research worker and can actually bring about the writing of new book scholar and publisher exist in symbiotic relationship which contains area of potential mutual benefit but also potential conflict good publisher are greatly appreciated by author who themselves become part of the communication network\n",
      "==================================================\n",
      "Stationäre Verteilungen endlicher offener Systeme \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Software validation \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Stabilization of bilinear systems by a linear feedback control In recent years, there has been considerable interest in bilinear systems [1, 2, 3] as appropriate mathematical model to represent the dynamical behaviour for a wide class of the engineering, biological, and economic systems. Recently many studies of the bilinear systems have been done from various points of view, e.g. controllability or observability, and a lot of problems connected with the optimal control have been solved.\n",
      "--------------------------------------------------\n",
      "stabilization of bilinear system by linear feedback control in recent year there ha been considerable interest in bilinear system a appropriate mathematical model to represent the dynamical behaviour for wide class of the engineering biological and economic system recently many study of the bilinear system have been done from various point of view eg controllability or observability and lot of problem connected with the optimal control have been solved\n",
      "==================================================\n",
      "Integrated data analysis and management for the problem solving environment Abstract Large quantities of application oriented data (e.g. measurement data, planning data, etc.) have been accumulated or are still under development in science and industry. The detailed analysis and non-routine usage of such data collections by typical endusers (scientists, engineers, economists—with little or no programming skill and ambitions) has heavy demand for 1. —a powerful interactive language 2. —a flexible data base management system 3. —easy embedding of application specific programs 4. —facilities for graphical data presentation 5. —online usage-information on programs and data 6. —application oriented description of data and programs The Integrated Data Analysis and Management System (IDAMS) which combines several components to satisfy all the above requirements has been largely implemented in APL. IDAMS offers access to data (managed by a data base system) and to programs (from a library of APL and non APL programs) with a simple but powerful query language. The attributes of data and programs are specified in a dictionary and their meaning is described in an information network of the interactive user guidance component. User guidance and adaptivity of the IDAMS interface to the user's skill are the key to easy and comfortable usage of IDAMS for problems solving by non DP professionals.\n",
      "--------------------------------------------------\n",
      "integrated data analysis and management for the problem solving environment abstract large quantity of application oriented data eg measurement data planning data etc have been accumulated or are still under development in science and industry the detailed analysis and non routine usage of such data collection by typical endusers scientist engineer economistswith little or no programming skill and ambition ha heavy demand for powerful interactive language flexible data base management system easy embedding of application specific program facility for graphical data presentation online usage information on program and data application oriented description of data and program the integrated data analysis and management system idams which combine several component to satisfy all the above requirement ha been largely implemented in apl idams offer access to data managed by data base system and to program from library of apl and non apl program with simple but powerful query language the attribute of data and program are specified in dictionary and their meaning is described in an information network of the interactive user guidance component user guidance and adaptivity of the idams interface to the user skill are the key to easy and comfortable usage of idams for problem solving by non dp professional\n",
      "==================================================\n",
      "Erfahrungen mit Spezifikationsmehtoden beim Programmkonstruktionspraktikum \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Directions in library networking Bibliographic control before and after MARC is reviewed. The capability of keying into online systems has brought an interdependence among libraries, the service centers that mediate between them, and the large utilities that process and distribute data. From this has developed the basic network structure among libraries in the United States. The independent development of major networks has brought problems in standardization and coordination. The authors point out that while technology has led toward centralization of automated library services, new developments are now pushing toward decentralization. Coordination is a requirement to avoid fragmentation in this new environment.\n",
      "--------------------------------------------------\n",
      "direction in library networking bibliographic control before and after marc is reviewed the capability of keying into online system ha brought an interdependence among library the service center that mediate between them and the large utility that process and distribute data from this ha developed the basic network structure among library in the united state the independent development of major network ha brought problem in standardization and coordination the author point out that while technology ha led toward centralization of automated library service new development are now pushing toward decentralization coordination is requirement to avoid fragmentation in this new environment\n",
      "==================================================\n",
      "Software engineering education Several studies have indicated that the data processing industry's most critical problem during the 1980's will be a shortage of qualified software engineers. This panel brings together a number of people who have been actively addressing the problem of increasing the supply of qualified software engineers through both University and industry programs in software engineering education. They will address the following issues:  • What skills will be needed by the software engineer of the future?  • What software engineering programs are currently underway in universities, industry, and professional societies to meet these needs?  • What particular approaches have been tried to date, and how well have they worked out?  • What are some resulting guidelines for mounting a successful software engineering education program?\n",
      "--------------------------------------------------\n",
      "software engineering education several study have indicated that the data processing industry most critical problem during the 1980 will be shortage of qualified software engineer this panel brings together number of people who have been actively addressing the problem of increasing the supply of qualified software engineer through both university and industry program in software engineering education they will address the following issue what skill will be needed by the software engineer of the future what software engineering program are currently underway in university industry and professional society to meet these need what particular approach have been tried to date and how well have they worked out what are some resulting guideline for mounting successful software engineering education program\n",
      "==================================================\n",
      "Analyzing Intention in Utterances This paper describes a model of cooperative behavior and describes how such a model can be applied in a natural language understanding system. We assume that agents attempt to recognize the plans of other agents and, then, use this plan when deciding what response to make. In particular, we show that, given a setting in which purposeful dialogues occur, this model can account for responses that provide more information that explicitly requested and for appropriate responses to both short sentence fragments and indirect speech acts.\n",
      "--------------------------------------------------\n",
      "analyzing intention in utterance this paper describes model of cooperative behavior and describes how such model can be applied in natural language understanding system we assume that agent attempt to recognize the plan of other agent and then use this plan when deciding what response to make in particular we show that given setting in which purposeful dialogue occur this model can account for response that provide more information that explicitly requested and for appropriate response to both short sentence fragment and indirect speech act\n",
      "==================================================\n",
      "Multivalued I2L Circuits for TSC Checkers We present a TSC multivalued I2L comparator which uses multivalued current inputs and two binary voltage outputs. This circuit is self-testing and fault-secure for single faults (either \"stuck-at\" or \"skew\" faults). It is the basic circuit to realize TSC checkers for nonseparable or separable codes. The schemes are simpler than the designs of the TSC combinational checkers.\n",
      "--------------------------------------------------\n",
      "multivalued i2l circuit for tsc checker we present tsc multivalued i2l comparator which us multivalued current input and two binary voltage output this circuit is self testing and fault secure for single fault either stuckdashat or skew fault it is the basic circuit to realize tsc checker for nonseparable or separable code the scheme are simpler than the design of the tsc combinational checker\n",
      "==================================================\n",
      "Review of \"Principles of Artificial Intelligence by Nils J. Nilsson\", Tioga Publishing Co. Nils Nilsson's new book, (Principles of Artificial Intelligence) (Tioga Publishing Co., 1980) discusses some basic ideas underlying different applications of AI. The book, designed as a text for a senior or first-year graduate course, aims to fill a gap between theory and practice. It succeeds in building a good solid arch outward from the shore of theory, but only a few support beams anchor the bridge to the shore of practice.\n",
      "--------------------------------------------------\n",
      "review of principle of artificial intelligence by nil nilsson tioga publishing co nil nilsson new book principle of artificial intelligence tioga publishing co 1980 discus some basic idea underlying different application of ai the book designed a text for senior or first year graduate course aim to fill gap between theory and practice it succeeds in building good solid arch outward from the shore of theory but only few support beam anchor the bridge to the shore of practice\n",
      "==================================================\n",
      "A microprogrammable dual processor based fast digital filter A programmable digital signal processing system is proposed in this paper. The system utilizes a 16 bit microprocessor for user interaction and a bit slice microprocessor for executing the filtering algorithm.\n",
      "--------------------------------------------------\n",
      "microprogrammable dual processor based fast digital filter programmable digital signal processing system is proposed in this paper the system utilizes 16 bit microprocessor for user interaction and bit slice microprocessor for executing the filtering algorithm\n",
      "==================================================\n",
      "A Logical Approach to the Problem \"P=NP?\" INTRODUCTION. A possible approach to the problem \"P~P?\" or to the problem of exponential lower time complexity bound for NP-complete sets or, more generally and informally, to the so called enume_____ra_tion problem [I, 8S ~ consists in the analysis of those logical means and principles that the problem depends on. This may be I) a technical analysis of logical and mathematical means and 2) an analysis of mathematical abstractions connected with the problem. The first attempt of such a technical logical analysis concerned with \"P~NP?\" was undertaken by J~artmanis and J.E~opcroft [6J. They proved that in any sufficiently strong axiomatic system Ax some version (P=~P)Ax of P=NP is independent of Ax. Note that the original assertion P=~P may also be written down in the languageAx, and in the formulation of (P=NP)Ax the prcvability predicate PrfAx is used in some way. From an external point of view both assertions are equivalent, and if this equivalence were provable in Ax, then we should have the independence of P~NP itself in the ordinary sense. However, this, obviously, cannot be made for all Ax. There is also no hope to prove such equivalence for some fixed Ax. So, independence of the original problem is not proved. The Main Result of this paper states, for some natural theory T, unprovability of the assertion on exponential lower time complexity bound of acceptin6 NP-sets (Theorems I. I, I. ~ the result announced iu [131). But, as opposed to E6] a direct formulation of the problem is used. Since the theory T is rather weak, this result may only be considered as a partial solution of the problem in terms of axiomatic independence. However, the difficulty of proving the Main Result ( §5; compare with 3.8, 3.9 and §6) says that T is strong enough.\n",
      "--------------------------------------------------\n",
      "logical approach to the problem np introduction possible approach to the problem or to the problem of exponential lower time complexity bound for npdashcomplete set or more generally and informally to the so called enume_____ra_tion problem 8 consists in the analysis of those logical mean and principle that the problem depends on this may be technical analysis of logical and mathematical mean and an analysis of mathematical abstraction connected with the problem the first attempt of such technical logical analysis concerned with np wa undertaken by artmanis and je opcroft 6j they proved that in any sufficiently strong axiomatic system ax some version ax of np is independent of ax note that the original assertion may also be written down in the languageax and in the formulation of np ax the prcvability predicate prfax is used in some way from an external point of view both assertion are equivalent and if this equivalence were provable in ax then we should have the independence of np itself in the ordinary sense however this obviously cannot be made for all ax there is also no hope to prove such equivalence for some fixed ax so independence of the original problem is not proved the main result of this paper state for some natural theory unprovability of the assertion on exponential lower time complexity bound of acceptin6 npdashsets theorem the result announced iu 131 but a opposed to e6 direct formulation of the problem is used since the theory is rather weak this result may only be considered a partial solution of the problem in term of axiomatic independence however the difficulty of proving the main result compare with 3point8 3point9 and say that is strong enough\n",
      "==================================================\n",
      "Some Consequences of a Result of Ehrenfeucht and Rozenberg — Recently, Ehrenfeucht and Rozenberg have proved that there are context-free languages that are not EDTOL languages. In the present note, some conséquences of this resuit are pointed out with référence to certain open problems related to matrix languages. Résumé. — Récemment, Ehrenfeucht et Rozenberg ont prouvé qu'il existe des langages indépendants du contexte qui ne sont pas des EDTOL-langages. Dans la présente note, nous indiquons quelques conséquences de ce résultat, en résolvant certains problèmes ouverts sur les langages matriciels.\n",
      "--------------------------------------------------\n",
      "some consequence of result of ehrenfeucht and rozenberg recently ehrenfeucht and rozenberg have proved that there are context free language that are not edtol language in the present note some consequence of this resuit are pointed out with reference to certain open problem related to matrix language resume recemment ehrenfeucht et rozenberg ont prouve qu il existe de langages independants du contexte qui ne sont pa de edtol langages dans la presente note nous indiquons quelques consequence de ce resultat en resolvant certains problemes ouverts sur le langages matriciels\n",
      "==================================================\n",
      "Digital analysis of ESR spectra of spin labeled nucleic acid- systems Abstract Programs written in FORTRAN II have been developed for analyzing ESR spectra of spin labeled nucleic acid ligand complexes. The spectra are aligned and standardized with respect to an external standard for studying parameters of nucleic acid ligand complexes by taking into account the entire spectral arrays. Results obtained on spin labeled nucleic acids and poly-L-lysine systems indicate the existence of a two component system with two kinds of apparent spin mobilities.\n",
      "--------------------------------------------------\n",
      "digital analysis of esr spectrum of spin labeled nucleic acid system abstract program written in fortran ii have been developed for analyzing esr spectrum of spin labeled nucleic acid ligand complex the spectrum are aligned and standardized with respect to an external standard for studying parameter of nucleic acid ligand complex by taking into account the entire spectral array result obtained on spin labeled nucleic acid and poly ldashlysine system indicate the existence of two component system with two kind of apparent spin mobility\n",
      "==================================================\n",
      "Algorithm 556: Exponential Integrals [S13] DESCRIPTION The Fortran subroutine EXPINT given here is an implementation of [1]. EX-PINT has four machine-dependent parameters XCUT, XLIM, ETOL, and EU-LER which are set into DATA statements. XCUT is a breakpoint such that for x _ XCUT, the series is evaluated and for x > XCUT, the Miller algorithm is applied. XLIM is the approximate underflow limit for e-=, x _ 0, and ETOL is nominally set to 1.E-D where D is the number of base 10 digits in a word. would be appropriate for IBM single-precision arithmetic. The two choices for XCUT reflect the fact that there is a loss of up to two digits on 1 < x _< 2 with the series evaluation. This loss can be tolerated on longer word length machines, but not on shorter word length machines. Maximum accuracy can always be achieved with XCUT = 1. However, for longer word lengths where D = 14, the reduction in computation by moving XCUT from 1 to 2, achieved at the expense of two digits of accuracy, seems to be a worthwhile trade-off, and D ffi 12 reflects this modification for CDC machines. D = 12 is also more consistent with the accuracy attainable from E X P (-X) near the underflow limit X = 667. While the subroutine EXPINT is almost portable, the function DIGAM, which computes the psi function at integer arguments, is supplied as a CDC 6600-7600 Fortran function. Modifications for other machines can be made easily from eq. Algorithms • 421 T h e convergence of eq. (2) in [1] is so rapid that the m = n-1 term, which requires ~(n), is reached for only small values of n. A table of 100 values suffices for virtually all single-precision implementations of E X P I N T. An initialization step to generate a higher precision table might be appropriate if multiple precision is anticipated. For CDC single precision or IBM double precision, 36 values suffice for n as high as 10 le and relative errors of 10-14. was used to check out the routine on parameter ranges were also made at tolerances T O L as low as 10-9. The quadrature~could be relied upon to give the requested accuracy down to T O L-10-1~ over the full exponent range of the CDC 6600. For large x, argument reduction in computing e-~ will result in decreased accuracy, up to three …\n",
      "--------------------------------------------------\n",
      "algorithm 556 exponential integral s13 description the fortran subroutine expint given here is an implementation of exdashpint ha four machine dependent parameter xcut xlim etol and eudashler which are set into data statement xcut is breakpoint such that for xcut the series is evaluated and for xcut the miller algorithm is applied xlim is the approximate underflow limit for and etol is nominally set to edashd where is the number of base 10 digit in word would be appropriate for ibm single precision arithmetic the two choice for xcut reflect the fact that there is loss of up to two digit on with the series evaluation this loss can be tolerated on longer word length machine but not on shorter word length machine maximum accuracy can always be achieved with xcut however for longer word length where 14 the reduction in computation by moving xcut from to achieved at the expense of two digit of accuracy seems to be worthwhile trade off and ffi 12 reflects this modification for cdc machine 12 is also more consistent with the accuracy attainable from near the underflow limit 667 while the subroutine expint is almost portable the function digam which computes the psi function at integer argument is supplied a cdc 6600 7600 fortran function modification for other machine can be made easily from eq algorithm 421 convergence of eq in is so rapid that the ndash1 term which requires is reached for only small value of table of 100 value suffices for virtually all single precision implementation of an initialization step to generate higher precision table might be appropriate if multiple precision is anticipated for cdc single precision or ibm double precision 36 value suffice for a high a 10 le and relative error of 10dash14 wa used to check out the routine on parameter range were also made at tolerance a low a 10dash9 the quadrature could be relied upon to give the requested accuracy down to ldash10dash1 over the full exponent range of the cdc 6600 for large argument reduction in computing will result in decreased accuracy up to three\n",
      "==================================================\n",
      "Growth Functions of Stochastic Lindenmayer Systems A stochastic version of the 0L systems of Lindenmayer is introduced and the growth functions of such systems are studied. Topics covered include techniques for computing such growth functions, classification of such stochastic 0L systems according to growth rate, and decidability results about various notions of growth equivalence.\n",
      "--------------------------------------------------\n",
      "growth function of stochastic lindenmayer system stochastic version of the 0l system of lindenmayer is introduced and the growth function of such system are studied topic covered include technique for computing such growth function classification of such stochastic 0l system according to growth rate and decidability result about various notion of growth equivalence\n",
      "==================================================\n",
      "Functional completeness and non-Łukasiewiczian truth functions \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Image processing NCC '80 has addressed three important areas of image processing through individually organized sessions. These topics are medical imaging, facsimile transmission, and image understanding. In addition, the panel discussion organized by T. Wiener addresses various current topics.\n",
      "--------------------------------------------------\n",
      "image processing ncc 80 ha addressed three important area of image processing through individually organized session these topic are medical imaging facsimile transmission and image understanding in addition the panel discussion organized by wiener address various current topic\n",
      "==================================================\n",
      "An integer linear programming approach to the steiner problem in graphs Consider a connected undirected graph G[N; E] with N = S ∪ P, the set of nodes, where P is designated as the set of Steiner points. A weight is associated with each edge ei of the set E. The problem of obtaining a minimal weighted tree which spans the set S of nodes has been termed in literature as the Steiner problem in graphs. A specialized integer programming (set covering) formulation is presented for the problem. The number of constraints in this formulation grows exponentially with the size of the problem. A method called the row generation scheme is developed to solve the above problem. The method requires knowing the constraints only implicitly. Several other problems which can be put in a similar framework can also be handled by the above scheme. The generality of the scheme and its efficiency is discussed. Finally the computational result is demonstrated.\n",
      "--------------------------------------------------\n",
      "an integer linear programming approach to the steiner problem in graph consider connected undirected graph with the set of node where is designated a the set of steiner point weight is associated with each edge ei of the set the problem of obtaining minimal weighted tree which span the set of node ha been termed in literature a the steiner problem in graph specialized integer programming set covering formulation is presented for the problem the number of constraint in this formulation grows exponentially with the size of the problem method called the row generation scheme is developed to solve the above problem the method requires knowing the constraint only implicitly several other problem which can be put in similar framework can also be handled by the above scheme the generality of the scheme and it efficiency is discussed finally the computational result is demonstrated\n",
      "==================================================\n",
      "Codes: Unequal Probabilities, Unequal Letter Cost The construction of alphabetic prefix codes with unequal letter costs and unequal probabilities is considered. A variant of the noiseless coding theorem is proved giving closely matching lower and upper bounds for the cost of the optimal code. Furthermore, an algorithm is described which constructs a nearly optimal code in linear time.\n",
      "--------------------------------------------------\n",
      "code unequal probability unequal letter cost the construction of alphabetic prefix code with unequal letter cost and unequal probability is considered variant of the noiseless coding theorem is proved giving closely matching lower and upper bound for the cost of the optimal code furthermore an algorithm is described which construct nearly optimal code in linear time\n",
      "==================================================\n",
      "A New and Efficient Algorithm for a Class of Portfolio Selection Problems This paper proposes a new approach and develops an efficient algorithm for solving a class of (simplified) portfolio selection problems. The approach is based on the technique of parametric principal pivoting. The algorithm is particularly suited for problems with special structure and can handle potentially large problems. When specialized to the multiple index model, the algorithm achieves enormous savings in computer storage and computations.\n",
      "--------------------------------------------------\n",
      "new and efficient algorithm for class of portfolio selection problem this paper proposes new approach and develops an efficient algorithm for solving class of simplified portfolio selection problem the approach is based on the technique of parametric principal pivoting the algorithm is particularly suited for problem with special structure and can handle potentially large problem when specialized to the multiple index model the algorithm achieves enormous saving in computer storage and computation\n",
      "==================================================\n",
      "A note on the stack size of regularly distributed binary trees Assume that in one unit of time a node is stored in the stack or is removed from the top of the stack during postorder-traversing of a binary tree. If all binary trees are equally likely the average stack size aftert units of time and the variance is computed as a function of the proportionϱ=t/n.\n",
      "--------------------------------------------------\n",
      "note on the stack size of regularly distributed binary tree assume that in one unit of time node is stored in the stack or is removed from the top of the stack during postorder traversing of binary tree if all binary tree are equally likely the average stack size aftert unit of time and the variance is computed a function of the proportion\n",
      "==================================================\n",
      "Continuation-based multiprocessing Any multiprocessing facility must include three features: elementary exclusion, data protection, and process saving. While elementary exclusion must rest on some hardware facility (e.g. a test-and-set instruction), the other two requirements are fulfilled by features already present in applicative languages. Data protection may be obtained through the use of procedures (closures or funargs),and process saving may be obtained through the use of the CATCH operator. The use of CATCH, in particular, allows an elegant treatment of process saving.  We demonstrate these techniques by writing the kernel and some modules for a multiprocessing system. The kernel is very small. Many functions which one would normally expect to find inside the kernel are completely decentralized. We consider the implementation of other schedulers, interrupts, and the implications of these ideas for language design.\n",
      "--------------------------------------------------\n",
      "continuation based multiprocessing any multiprocessing facility must include three feature elementary exclusion data protection and process saving while elementary exclusion must rest on some hardware facility eg test and set instruction the other two requirement are fulfilled by feature already present in applicative language data protection may be obtained through the use of procedure closure or funargs and process saving may be obtained through the use of the catch operator the use of catch in particular allows an elegant treatment of process saving we demonstrate these technique by writing the kernel and some module for multiprocessing system the kernel is very small many function which one would normally expect to find inside the kernel are completely decentralized we consider the implementation of other scheduler interrupt and the implication of these idea for language design\n",
      "==================================================\n",
      "The Decidability of Persistence for Vector Addition Systems \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Tuning the coalesced hashing method to obtain optimum performance This paper analyzes the coalesced hashing method, in which a portion of memory (called the address region) serves as the range of the hash function while the rest of memory (called the cellar) is devoted solely to storing records that collide when inserted. If the cellar should get full, subsequent colliders must be stored in empty slots in the address region and, thus, may cause later collisions. Varying the relative size of the cellar affects search performance. The main result of this paper expresses the average search times as a function of the number of records and the cellar size, solving the long-standing open problem described in [Knu73, §6.4-43]. We use these formulas to pick the cellar size that leads to optimum search performance and then show that this \"tuned\" method is competitive with several well-known hashing schemes.\n",
      "--------------------------------------------------\n",
      "tuning the coalesced hashing method to obtain optimum performance this paper analyzes the coalesced hashing method in which portion of memory called the address region serf a the range of the hash function while the rest of memory called the cellar is devoted solely to storing record that collide when inserted if the cellar should get full subsequent collider must be stored in empty slot in the address region and thus may cause later collision varying the relative size of the cellar affect search performance the main result of this paper express the average search time a function of the number of record and the cellar size solving the long standing open problem described in knu73 6point4dash43 we use these formula to pick the cellar size that lead to optimum search performance and then show that this tuned method is competitive with several well known hashing scheme\n",
      "==================================================\n",
      "A New Class of Error-Correcting/Detecting Codes for Fault-Tolerant Computer Applications Separable error-correcting/detecting codes are developed that provide protection against combinations of both unidirectional and random errors. Specifically, codes are presented which can both: 1) correct (detect) some t random errors, and 2) detect any number of unidirectional errors which may also contain t or fewer random errors. Necessary and sufficient conditions for the existence of these codes are also developed. Decoding algorithms for these codes are presented, and implementations of the algorithms are also discussed.\n",
      "--------------------------------------------------\n",
      "new class of error correcting detecting code for fault tolerant computer application separable error correcting detecting code are developed that provide protection against combination of both unidirectional and random error specifically code are presented which can both correct detect some random error and detect any number of unidirectional error which may also contain or fewer random error necessary and sufficient condition for the existence of these code are also developed decoding algorithm for these code are presented and implementation of the algorithm are also discussed\n",
      "==================================================\n",
      "An Optimizing Pascal Compiler The architecture of a production optimizing compiler for Pascal is described, and the structure of the optimizer is detailed. The compiler performs both interprocedural and global optimizations, in addition to optimization of basic blocks. We have found that a high-level structured language such as Pascal provides unique opportunities for effective optimization, but that standard optimization techniques must be extended to take advantage of these opportunities. These issues are considered in our discussion of the optimization algorithms we have developed and the sequence in which we apply them.\n",
      "--------------------------------------------------\n",
      "an optimizing pascal compiler the architecture of production optimizing compiler for pascal is described and the structure of the optimizer is detailed the compiler performs both interprocedural and global optimization in addition to optimization of basic block we have found that high level structured language such a pascal provides unique opportunity for effective optimization but that standard optimization technique must be extended to take advantage of these opportunity these issue are considered in our discussion of the optimization algorithm we have developed and the sequence in which we apply them\n",
      "==================================================\n",
      "Structural Knowledge for the Recognition of Syllables in Continuous Speech \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "The Limits of Togetherness This paper makes a distinction between conversation and communication (signal transfer which may, or may not, be conversational) . The word \" conversation\" is given an interpretation, which ref.1nes its COIIBDonsense meaning. conversation maintains the autonor;Jy or identity of systems and, also , generates independencies between systems (human , societal , or others), which is a prerequisite of dialogue.\n",
      "--------------------------------------------------\n",
      "the limit of togetherness this paper make distinction between conversation and communication signal transfer which may or may not be conversational the word conversation is given an interpretation which ref 1nes it coiibdonsense meaning conversation maintains the autonor jy or identity of system and also generates independency between system human societal or others which is prerequisite of dialogue\n",
      "==================================================\n",
      "Music analysis by computer: A conference report \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "The Evaluation of an Automatically Indexed, Machine-Readable Chemical Reactions File \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Ein Ansatz für die Problemklassenstrukturierung im Dialog \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "A class of pbib-designs obtained from projective geometries In this paper, a new class of partially balanced incomplete block designs is constructed over an association scheme obtained from a finite projective geometry. Further, a general method for deriving a balanced incomplete block design from another one is given.\n",
      "--------------------------------------------------\n",
      "class of pbib design obtained from projective geometry in this paper new class of partially balanced incomplete block design is constructed over an association scheme obtained from finite projective geometry further general method for deriving balanced incomplete block design from another one is given\n",
      "==================================================\n",
      "Formal Properties Of Rule Orderings In Linguistics The discovery in the late 1960's that standard linguistic theory (of Chomsky's Aspects ) was equivalent in generative power to unrestricted rewrite rules caused linguists to search for a \"stronger linguistic metatheory\". It seemed to some of these researchers that this meant describing linguistic theory by means of rules which were more restricted than type 0 languages. Such a view we call the L-view of constraints on linguistic theory: it advocates constraining the allowable rules in such a way that legitimate grammars can no longer generate arbitrary r.e. sets, but only some subset of them. To other researchers this discovery meant rather that one should place restrictions on linguistic theory so that the kinds of grammars allowed would be limited, regardless of whether such limitations affected the generative power of the theory. We call this the G-view of constraints. The L- and G-views are not equivalent limitations. For example, a G-view limitation on the class of regular grammars that any legitimate grammar be right-embedding is not thereby a L-view limitation, since this does not effect an alteration in generative power of the grammars allowed. The G-view is avowedly psychological; according to it, the point of placing constraints on grammars is to lessen directly the language learner's burden of choosing the correct grammar from all the possible ones. For the L-view, this is a side effect of disallowing whole classes of grammars in the first place.\n",
      "--------------------------------------------------\n",
      "formal property of rule ordering in linguistics the discovery in the late 1960 that standard linguistic theory of chomsky aspect wa equivalent in generative power to unrestricted rewrite rule caused linguist to search for stronger linguistic metatheory it seemed to some of these researcher that this meant describing linguistic theory by mean of rule which were more restricted than type language such view we call the ldashview of constraint on linguistic theory it advocate constraining the allowable rule in such way that legitimate grammar can no longer generate arbitrary re set but only some subset of them to other researcher this discovery meant rather that one should place restriction on linguistic theory so that the kind of grammar allowed would be limited regardless of whether such limitation affected the generative power of the theory we call this the gdashview of constraint the and gdashviews are not equivalent limitation for example gdashview limitation on the class of regular grammar that any legitimate grammar be right embedding is not thereby ldashview limitation since this doe not effect an alteration in generative power of the grammar allowed the gdashview is avowedly psychological according to it the point of placing constraint on grammar is to lessen directly the language learner burden of choosing the correct grammar from all the possible one for the ldashview this is side effect of disallowing whole class of grammar in the first place\n",
      "==================================================\n",
      "Geschwindigkeitsmessung mit Hilf zeitdifferenzierter Laser-Speckles Mit Hilfe statistischer Methoden erster Ordnung last sich ein sehr effektives Verfahren zur Messung der Geschwindigkeit von lichtstreuenden Objekten realisieren. Im folgenden werden der quantitative Zusammenhang zwischen Geschwindigkeit und Varianz des zeitlichen Differentialquotienten der Intensitat im Streufeld sowie erste Mesergebnisse prasentiert.\n",
      "--------------------------------------------------\n",
      "geschwindigkeitsmessung mit hilf zeitdifferenzierter laser speckle mit hilfe statistischer methoden erster ordnung last sich ein sehr effektives verfahren zur messung der geschwindigkeit von lichtstreuenden objekten realisieren im folgenden werden der quantitative zusammenhang zwischen geschwindigkeit und varianz de zeitlichen differentialquotienten der intensitat im streufeld sowie erste mesergebnisse prasentiert\n",
      "==================================================\n",
      "Complex digital control systems: Guthikonda V. Rayo, Van Nostrand Reinhold (1979), 516 pp, £27.40 \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Proceedings of the 1st International Conference on Information Systems, ICIS 1980, Philadelphia, Pennsylvania, USA, 1980 \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "The Design and Application of a Retargetable Peephole Optimizer Peephole optimizers improve object code by replacing certain sequences of instructions with better sequences. This paper describes PO, a peephole optimizer that uses a symbolic machine description to simulate pairs of adjacent instructions, replacing them, where possible, with an equivalent single instruction. As a result of this organization, PO is machine independent and can be described formally and concisely: when PO is finished, no instruction, and no pair of adjacent instructions, can be replaced with a cheaper single instruction that has the same effect. This thoroughness allows PO to relieve code generators of much case analysis; for example, they might produce only load/add-register sequences and rely on PO to, where possible, discard them in favor or add-memory, add-immediate, or increment instructions. Experiments indicate that naive code generators can give good code if used with PO.\n",
      "--------------------------------------------------\n",
      "the design and application of retargetable peephole optimizer peephole optimizers improve object code by replacing certain sequence of instruction with better sequence this paper describes po peephole optimizer that us symbolic machine description to simulate pair of adjacent instruction replacing them where possible with an equivalent single instruction a result of this organization po is machine independent and can be described formally and concisely when po is finished no instruction and no pair of adjacent instruction can be replaced with cheaper single instruction that ha the same effect this thoroughness allows po to relieve code generator of much case analysis for example they might produce only load add register sequence and rely on po to where possible discard them in favor or add memory add immediate or increment instruction experiment indicate that naive code generator can give good code if used with po\n",
      "==================================================\n",
      "Prefix classes of krom formulae with identity Two small classes of first order formulae without function symbols but with identity, in prenex conjunctive normal form with all disjunctions binary, are shown to have a recursively unsolvable decision problem, whereas for another such class an algorithm is developed which solves the decision problem of that class. This solves the prefix problem for such classes of formulae except for the Gödel-Kalmàr-Schütte case.ZusammenfassungFür zwei Klassen erststufiger Formeln in pränexer konjunktiver Normalform mit Identität aber ohne Funktionssymbole wird das Entscheidungsproblem als rekursiv unlösbar nachgewiesen. Für eine weitere solche Ausdrucksklasse wird ein Algorithmus zur Lösung des Entscheidungsproblems angegeben. Bis auf den Gödel-Kalmàr-Schütte-Fall löst dies das Präfixproblem für derartige Ausdrucksklassen.\n",
      "--------------------------------------------------\n",
      "prefix class of krom formula with identity two small class of first order formula without function symbol but with identity in prenex conjunctive normal form with all disjunction binary are shown to have recursively unsolvable decision problem whereas for another such class an algorithm is developed which solves the decision problem of that class this solves the prefix problem for such class of formula except for the godel kalmar schutte case zusammenfassungfur zwei klassen erststufiger formeln in pranexer konjunktiver normalform mit identitat aber ohne funktionssymbole wird da entscheidungsproblem al rekursiv unlosbar nachgewiesen fur eine weitere solche ausdrucksklasse wird ein algorithmus zur losung de entscheidungsproblems angegeben bi auf den godel kalmar schutte fall lost dy da prafixproblem fur derartige ausdrucksklassen\n",
      "==================================================\n",
      "Attributes of parallel and cascade microprocessor implementations of digital signal processing Microprocessors have achieved great popularity because they are inexpensive, convenient, and flexible. A microprocessor with an architecture chosen to efficiently perform digital signal processing algorithms would retain these advantages when used to implement digital signal processing. One such architecture is the Research Signal Processor (RSP) which was designed by Abraham Peled and constructed at the IBM Thomas J. Watson Research Center. In order for the RSP to be cost effective, however, it must be capable of implementing a wide range of problems so that the cost of developing an LSI version can be borne by many applications. The potential applications, however, vary greatly in size. A microprocessor capable of implementing the largest of these applications would be expensive and underutilized for many others. The approach we have chosen is to implement modestly sized applications with a single RSP, and to distribute larger applications over several RSPs. In the hope of keeping the cost of this distribution small, two simple structures, the parallel and cascade clusters were first investigated. This paper presents the attributes of these structures when used to implement the FFT, FIR filters and IIR filters. The types of data transfers required to implement these algorithms are also found, and the I/O capabilities of the RSP chosen to perform these transfers are presented.\n",
      "--------------------------------------------------\n",
      "attribute of parallel and cascade microprocessor implementation of digital signal processing microprocessor have achieved great popularity because they are inexpensive convenient and flexible microprocessor with an architecture chosen to efficiently perform digital signal processing algorithm would retain these advantage when used to implement digital signal processing one such architecture is the research signal processor rsp which wa designed by abraham peled and constructed at the ibm thomas watson research center in order for the rsp to be cost effective however it must be capable of implementing wide range of problem so that the cost of developing an lsi version can be borne by many application the potential application however vary greatly in size microprocessor capable of implementing the largest of these application would be expensive and underutilized for many others the approach we have chosen is to implement modestly sized application with single rsp and to distribute larger application over several rsps in the hope of keeping the cost of this distribution small two simple structure the parallel and cascade cluster were first investigated this paper present the attribute of these structure when used to implement the fft fir filter and iir filter the type of data transfer required to implement these algorithm are also found and the capability of the rsp chosen to perform these transfer are presented\n",
      "==================================================\n",
      "Localisation de valeurs propres et calcul de sous-espaces invariants \n",
      "--------------------------------------------------\n",
      "NOT REASONABLE\n",
      "==================================================\n",
      "Promotion of information science research by the government of the Federal Republic of Germany “Progress in research and technolo~cal development, in industry, administration and politics depends decisively on the extent to which information and knowledge can be made available quickly, in a compact and relevant form, and practically applied. . . . Therefore multi-disciplinary information units, in which the information is collected and processed from every relevant point of view, are necessary. . . . Moreover, the infrastructure of information and documentation must be reorganized and improved by better and more systematic promotion of research, standardization, and education. . . . The activities of the Federal Government for the promotion of information and documentation rest on the assumption that the setting up of an effective information system is essentially a public responsibility. . ,” (from the foreword of the Programme of the Federal Government for the Promotion of Information and Documentation). As a consequence the promotion of I&D is an essential part of promotion of research and technology, and therefore it must be seen in co~nec~~o~ Ajax the policy for research and technology of the Federal Government. The following chart gives a general view on the aims and priorities of the German policy on research and technology. The share of grants of the Federal Government for I&D in 1979 amounted to about 1% of the total expenses for the development of science,\n",
      "--------------------------------------------------\n",
      "promotion of information science research by the government of the federal republic of germany progress in research and technolo cal development in industry administration and politics depends decisively on the extent to which information and knowledge can be made available quickly in compact and relevant form and practically applied therefore multi disciplinary information unit in which the information is collected and processed from every relevant point of view are necessary moreover the infrastructure of information and documentation must be reorganized and improved by better and more systematic promotion of research standardization and education the activity of the federal government for the promotion of information and documentation rest on the assumption that the setting up of an effective information system is essentially public responsibility from the foreword of the programme of the federal government for the promotion of information and documentation a consequence the promotion of is an essential part of promotion of research and technology and therefore it must be seen in co nec ajax the policy for research and technology of the federal government the following chart give general view on the aim and priority of the german policy on research and technology the share of grant of the federal government for in 1979 amounted to about of the total expense for the development of science\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(250,300):\n",
    "    title = lines[i]['title']\n",
    "    abstract = lines[i]['paperAbstract']\n",
    "    if reasonable(title, abstract):\n",
    "        text = title + ' ' + abstract\n",
    "        print(text)\n",
    "        print('-'*50)\n",
    "        print(clean(text, wnl, tokeniser))\n",
    "    else:\n",
    "        text = title + ' ' + abstract\n",
    "        print(text)\n",
    "        print('-'*50)\n",
    "        print('NOT REASONABLE')\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yield model for productivity optimization of vlsi memory chip with redundancy and partially good product model with mixed poisson statistic ha been developed for calculating the yield for memory chip with redundant line and for partially good product the mixing process requires two parameter which are readily obtained from product data the product is described in the model by critical area which depend on the circuit sensitivity to defect and they can be determined in systematic way the process is represented in the model by defect density and gross yield loss these are measured with defect monitor independently of product type this paper show how the yield for any product can be calculated given the critical area defect density and mixing parameter future yield are forecast by using expected improvement in defect density example show good agreement between actual and calculated yield'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(text, wnl, tokeniser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean real data\n",
    "\n",
    "- In this section, we go through each year file in semantic_scholar_filtered and clean the data. New year files to be stored in semantic_scholar_clean. One document per line [title and abstract are together]. The data is now preparared for vectorisation and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n"
     ]
    }
   ],
   "source": [
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()\n",
    "\n",
    "\n",
    "for year in range(1980, 2022):\n",
    "    cleaned_text = []\n",
    "    print(year)\n",
    "    f = open(\"../../Data/semantic_scholar_filtered/\"+str(year)+\".txt\", \"r\", encoding=\"utf8\")\n",
    "    for i, line in enumerate(f):\n",
    "        line = json.loads(line)\n",
    "        \n",
    "        title = line['title']\n",
    "        abstract = line['paperAbstract']\n",
    "        if reasonable(title, abstract):\n",
    "            text = title + ' ' + abstract\n",
    "            cleaned_text.append(clean(text, wnl, tokeniser))\n",
    "            \n",
    "    \n",
    "    with open(\"../../Data/semantic_scholar_cleaned/\"+str(year)+\".txt\", \"a\") as f:\n",
    "        for line in cleaned_text:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
