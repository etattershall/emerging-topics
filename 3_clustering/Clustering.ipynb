{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this notebook, we assign a burstiness score to each term in the papers dataset, 2006-2020, then select the 1000 burstiest terms and cluster them. These clusters are then copied into the cluster_choice.xlsx spreadsheet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster import hierarchy\n",
    "import pickle\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append(\"../../tools\")\n",
    "\n",
    "import burst_detection\n",
    "import tools\n",
    "import my_parameters\n",
    "import logletlab\n",
    "\n",
    "import my_stopwords3\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import StrMethodFormatter, NullFormatter\n",
    "import matplotlib.ticker as mticker\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "def reverse_cumsum(ls):\n",
    "    reverse = np.zeros_like(ls)\n",
    "    for i in range(len(ls)):\n",
    "        if i == 0:\n",
    "            reverse[i] = ls[i]\n",
    "        else:\n",
    "            reverse[i] = ls[i]-ls[i-1]\n",
    "            \n",
    "            \n",
    "    if reverse[0]>reverse[1]:\n",
    "        reverse[0]=reverse[1]\n",
    "            \n",
    "    return reverse\n",
    "\n",
    "def detransform_fit(ypc, F):\n",
    "    '''\n",
    "    The Gompertz and Logistic curves actually model *cumulative* frequency over time, not raw frequency. \n",
    "    However, raw frequency is more intuitive for graphs, so we use this function to change a cumulative \n",
    "    time series into a non-cumulative one. Additionally, the models were originally fitted to scaled curves\n",
    "    (such that the minumum frequency was zero and the maximum was one). This was done to make it possible to \n",
    "    directly compare the error between different time series without a much more frequent term dwarfing the calculation.\n",
    "    We now transform back.\n",
    "    '''\n",
    "    yf = reverse_cumsum(F*(max(ypc)-min(ypc)) + min(ypc))\n",
    "    return yf\n",
    "\n",
    "stop = my_stopwords3.get_stopwords()\n",
    "stop.add('using')\n",
    "stop.add('use')\n",
    "stop.add('uses')\n",
    "stop.add('used')\n",
    "stop.add('model')\n",
    "stop.add('method')\n",
    "stop.add('approach')\n",
    "stop.add('based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "significance threshold: 0.0015\n",
      "years above significance: 3\n",
      "significance ma length: 3\n"
     ]
    }
   ],
   "source": [
    "years = list(range(2006, 2021))\n",
    "vocabulary = pickle.load(open(\"../vocabulary.p\", \"rb\"))\n",
    "stacked_vectors = pickle.load(open(\"../stacked_vectors/semantic_scholar.p\", \"rb\"))\n",
    "document_count_per_year = pickle.load(open(\"../stacked_vectors/semantic_scholar_document_count.p\", \"rb\"))\n",
    "\n",
    "prevalence = stacked_vectors.divide(document_count_per_year['documents'], axis=0).loc[years]\n",
    "parameters = my_parameters.set_parameters()\n",
    "\n",
    "print('significance threshold:', parameters['significance_threshold'])\n",
    "print('years above significance:', parameters['years_above_significance'])\n",
    "print('significance ma length:', parameters['significance_ma_length'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply burst detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bd_dataset = burst_detection.Dataset(\n",
    "    name = \"semantic_scholar\", \n",
    "    years = years, \n",
    "    stacked_vectors = prevalence\n",
    ")\n",
    "\n",
    "bd_dataset.get_sig_stacked_vectors(parameters[\"significance_threshold\"], parameters[\"years_above_significance\"])\n",
    "print(bd_dataset.sig_stacked_vectors.shape)\n",
    "\n",
    "bd_dataset.get_burstiness(parameters[\"short_ma_length\"], parameters[\"long_ma_length\"], parameters[\"significance_ma_length\"], parameters[\"signal_line_ma\"])\n",
    "\n",
    "bd_dataset.get_burstiness(parameters[\"short_ma_length\"], parameters[\"long_ma_length\"], parameters[\"significance_ma_length\"], parameters[\"signal_line_ma\"])\n",
    "bursts = tools.get_top_n_bursts(bd_dataset.burstiness, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate co-occurence of bursts\n",
    "\n",
    "There are two ways to do this\n",
    "1. Calculate based on co-occurence of bursty terms\n",
    "2. Calculate based on co-occurence of all terms\n",
    "\n",
    "In this case, I think the correct answer is 1. We want tight clusters of terms that are very co-related. Our clustering will also have a manual aspect, because my domain knowledge in this field means I can collapse trivial clusters into each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 42.46871376037598\n",
      "2007 48.72326636314392\n",
      "2008 46.577659130096436\n",
      "2009 52.749733448028564\n",
      "2010 55.083566665649414\n",
      "2011 61.355138063430786\n",
      "2012 65.35193872451782\n",
      "2013 69.37733364105225\n",
      "2014 73.18735694885254\n",
      "2015 77.63297581672668\n",
      "2016 100.17408037185669\n",
      "2017 126.26783609390259\n",
      "2018 97.63837885856628\n",
      "2019 115.65487456321716\n",
      "2020 106.81793999671936\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                             ngram_range=(1,4),\n",
    "                             stop_words=stop,\n",
    "                             vocabulary=bursts)\n",
    "\n",
    "vectors = []\n",
    "for year in years:\n",
    "    t0 = time.time()\n",
    "    with open(\"../../Data/semantic_scholar_cleaned_langdetect/\"+str(year)+\".txt\", \"r\") as f:\n",
    "        documents = f.readlines()\n",
    "        documents = [d.strip() for d in documents] \n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             ngram_range=(1,4),\n",
    "                             vocabulary=bursts,\n",
    "                             stop_words=stop\n",
    "                            )\n",
    "    \n",
    "    vector = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    del documents\n",
    "    \n",
    "    vector[vector>1] = 1    \n",
    "    vectors.append(vector)\n",
    "    \n",
    "    del vector\n",
    "    \n",
    "    print(year, time.time()-t0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmat\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "v = vectors[0]\n",
    "c = v.T*v\n",
    "c.setdiag(0)\n",
    "c = c.todense()\n",
    "\n",
    "cooccurrence = c\n",
    "\n",
    "for v in vectors[1:]:\n",
    "    c = v.T*v\n",
    "    c.setdiag(0)\n",
    "    c = c.todense()\n",
    "    cooccurrence += c\n",
    "    \n",
    "pickle.dump(cooccurrence, open('semantic_scholar_cooccurrence_matrix_2006.p', \"wb\"))\n",
    "pickle.dump(bursts, open('semantic_scholar_cooccurrence_vocabulary_2006.p', \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github, github com, http github com, http github, com|0, 0, 0, 0, 454|2020, 2020, 2020, 2020, 2020\n",
      "availability implementation, contact, edu, gene, protein, sequencing, sequence|6, 1033, 412, 2139, 1889, 335, 7874|2016, 2017, 2011, 2007, 2006, 2016, 2006\n",
      "web, linked data, linked, rdf, ontology, semantic, semantic web|9139, 17, 787, 282, 2816, 4644, 1208|2006, 2014, 2014, 2014, 2008, 2008, 2006\n",
      "knowledge graph|9|2020\n",
      "embeddings, embedding, word embeddings|106, 1025, 0|2020, 2020, 2019\n",
      "learn, representation learning, learns, learned, jointly, feature representation|1857, 14, 387, 1339, 743, 75|2020, 2020, 2020, 2020, 2020, 2020\n",
      "lstm, long short term, long short, long short term memory, short term memory, term memory, short term, memory lstm, term memory lstm, short term memory lstm, short, long|5, 12, 37, 7, 39, 98, 470, 3, 3, 3, 3532, 4570|2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020\n",
      "achieves state, achieves state ofdashthe, achieves state ofdashthe art|7, 6, 6|2020, 2020, 2020\n",
      "cnns, network cnns, neural network cnns, convolutional neural network cnns|31, 15, 14, 0|2020, 2020, 2020, 2020\n",
      "convolutional, convolutional neural, convolutional neural network, cnn, network cnn, neural network cnn, convolutional neural network cnn, deep convolutional, deep convolutional neural, deep convolutional neural network|227, 19, 19, 102, 53, 44, 6, 0, 0, 0|2020, 2020, 2020, 2020, 2020, 2020, 2020, 2019, 2019, 2019\n",
      "semantic segmentation, mask, wise|5, 417, 507|2020, 2020, 2020\n",
      "fully convolutional, convolutional network|0, 6|2019, 2020\n",
      "deep neural, deep neural network, dnns, dnn, network dnn, neural network dnn, deep neural network dnn|0, 0, 1, 6, 4, 4, 0|2020, 2020, 2020, 2020, 2020, 2020, 2020\n",
      "adversarial, generative, generative adversarial, adversarial network, generative adversarial network, gan|121, 414, 0, 1, 0, 41|2020, 2020, 2020, 2020, 2020, 2020\n",
      "pre trained, transfer learning, pre, transfer|14, 18, 2532, 2526|2020, 2020, 2020, 2020\n",
      "augmentation, data augmentation|169, 10|2020, 2020\n",
      "deep, neural, neural network, deep learning, training, trained, train, training data, deep network, network trained|896, 5066, 4160, 11, 4625, 1389, 848, 761, 1, 149|2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020\n",
      "autoencoder|0|2020\n",
      "compressive, compressive sensing, compressed sensing|28, 3, 13|2015, 2015, 2015\n",
      "sdn, software defined, software defined networking, defined networking, defined networking sdn, networking sdn, software defined networking sdn, defined|4, 86, 0, 0, 0, 0, 0, 6954|2018, 2017, 2017, 2017, 2018, 2018, 2018, 2006\n",
      "big data, big, analytics, data analytics, era|5, 567, 69, 4, 328|2018, 2018, 2017, 2018, 2020\n",
      "mapreduce, hadoop, data processing|4, 1, 367|2015, 2015, 2018\n",
      "cloud, cloud computing, cloud service, provider, cloud environment, computing, service provider|327, 7, 1, 1718, 2, 8283, 901|2017, 2014, 2015, 2014, 2017, 2018, 2015\n",
      "blockchain, contract, transaction|0, 544, 1347|2020, 2020, 2006\n",
      "smart, smart grid, electricity, electric vehicle, electric, power grid|1123, 2, 147, 27, 556, 62|2019, 2014, 2019, 2020, 2020, 2020\n",
      "smart city, city, citizen|2, 626, 312|2019, 2018, 2016\n",
      "4point0, cyber, cyber physical|53, 160, 1|2020, 2019, 2018\n",
      "ofdashthings, internet ofdashthings|0, 0|2020, 2020\n",
      "iot, internet thing, thing, internet thing iot, thing iot, iot device, internet|5, 10, 620, 1, 1, 0, 5399|2019, 2019, 2019, 2019, 2019, 2020, 2019\n",
      "android, smartphones, smartphone, app, apps|23, 22, 19, 52, 9|2016, 2015, 2016, 2017, 2016\n",
      "experience, user experience, quality experience|4817, 336, 16|2017, 2017, 2018\n",
      "kinect, rehabilitation|0, 138|2013, 2017\n",
      "crowdsourcing, crowd|0, 134|2016, 2016\n",
      "networking, facebook, online social, social networking, online social network, friend|1219, 5, 26, 40, 12, 203|2016, 2013, 2014, 2012, 2014, 2012\n",
      "twitter, social medium, tweet, medium, sentiment, opinion|1, 5, 1, 3748, 60, 602|2016, 2017, 2016, 2016, 2020, 2016\n",
      "tex, notation latex, math notation latex, tex math notation latex, tex math notation, formula tex math notation, math notation, tex math, inline formula tex, formula tex math, inline formula tex math, formula tex, inline formula, latex, inline, tex math inline, math inline formula, tex math inline formula, math inline, math, notation, notation latex tex, latex tex math, math notation latex tex, notation latex tex math, latex tex, latex tex math inline, formula|32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 39, 0, 0, 0, 0, 496, 558, 0, 0, 0, 0, 0, 0, 1989|2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020\n",
      "mu, mathrm|305, 10|2020, 2020\n",
      "energy harvesting, harvesting|18, 116|2018, 2018\n",
      "cognitive radio, cognitive, radio, primary user, secondary user, cognitive radio network, secondary, pu, radio network, spectrum sensing, spectrum, cr, primary|90, 1759, 2056, 18, 9, 12, 760, 28, 154, 11, 2047, 131, 2013|2012, 2014, 2012, 2013, 2013, 2014, 2012, 2013, 2012, 2013, 2014, 2013, 2014\n",
      "lte, term evolution, long term evolution, evolution|13, 34, 27, 3327|2014, 2014, 2014, 2014\n",
      "device todashdevice, todashdevice, d2d|3, 6, 4|2017, 2017, 2017\n",
      "cellular network, small cell, cellular, heterogeneous, cell, heterogeneous network, base station, base, tier|429, 12, 1958, 2976, 3252, 253, 735, 3949, 254|2016, 2017, 2016, 2015, 2015, 2015, 2017, 2017, 2017\n",
      "millimeter wave, millimeter, mmwave, wave, beam|48, 89, 1, 1330, 529|2019, 2019, 2020, 2020, 2020\n",
      "massive mimo, massive, pilot|0, 551, 849|2017, 2020, 2017\n",
      "noma, orthogonal multiple access, orthogonal multiple, non orthogonal|0, 3, 4, 49|2020, 2020, 2020, 2020\n",
      "edge computing, fog, low latency, latency|6, 25, 238, 1682|2020, 2019, 2020, 2020\n",
      "5g, fifth generation, 5g network, fifth, radio access|2, 2, 1, 155, 126|2020, 2020, 2020, 2020, 2017\n",
      "drone|2|2020\n",
      "autonomous driving, vehicle, autonomous, driving, autonomous vehicle, intelligent, traffic, driver|11, 1926, 2016, 832, 74, 2889, 4667, 759|2020, 2020, 2020, 2020, 2020, 2020, 2018, 2019\n",
      "reinforcement learning, reinforcement, deep reinforcement, deep reinforcement learning, rl, reward, markov decision|456, 554, 0, 0, 100, 384, 246|2020, 2020, 2020, 2020, 2020, 2020, 2020\n",
      "1000 224 100\n"
     ]
    }
   ],
   "source": [
    "cooccurrence = pickle.load(open('semantic_scholar_cooccurrence_matrix_2006.p', \"rb\"))\n",
    "bursts = pickle.load(open('semantic_scholar_cooccurrence_vocabulary_2006.p', \"rb\"))\n",
    "\n",
    "\n",
    "# Translate co-occurence into a distance\n",
    "dists = np.log(cooccurrence+1).max()- np.log(cooccurrence+1)\n",
    "\n",
    "# Remove the diagonal (squareform requires diagonals be zero)\n",
    "dists -= np.diag(np.diagonal(dists))\n",
    "\n",
    "# Put the distance matrix into the format required by hierachy.linkage\n",
    "flat_dists = squareform(dists)\n",
    "\n",
    "# Get the linkage matrix\n",
    "linkage_matrix = hierarchy.linkage(flat_dists, \"ward\")\n",
    "\n",
    "assignments = hierarchy.fcluster(linkage_matrix, 7, 'distance')\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for term, assign, co in zip(bursts, assignments, cooccurrence):\n",
    "    clusters[assign].append(term)\n",
    "\n",
    "\n",
    "for key in sorted(clusters.keys()):\n",
    "    terms = [t for t in clusters[key]]\n",
    "    total+=len(terms)\n",
    "    n2006 = [stacked_vectors[t][2006] for t in clusters[key]]\n",
    "    peak = [prevalence[t].idxmax() for t in clusters[key]]\n",
    "    \n",
    "    if min(n2006) > 20:\n",
    "        # Ignore bursts\n",
    "        pass\n",
    "    elif max(peak) < 2008:\n",
    "        pass\n",
    "    else:\n",
    "        tally+=1\n",
    "        print( ', '.join(clusters[key])+'|'+\n",
    "              ', '.join([str(t) for t in n2006])+'|'+\n",
    "              ', '.join([str(t) for t in peak]))\n",
    "print(total, len(clusters), tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters = [\n",
    "['knowledge graph'],\n",
    "['word embeddings'],\n",
    "['lstm', 'long short term memory'],\n",
    "['semantic segmentation'],\n",
    "['deep neural', 'dnn'],\n",
    "['generative adversarial', 'gan'],\n",
    "['autoencoder'],\n",
    "['compressive sensing', 'compressed sensing'],\n",
    "['sdn', 'software defined networking'],\n",
    "['big data'],\n",
    "['mapreduce', 'hadoop'],\n",
    "['cloud computing', 'cloud service', 'cloud environment'],\n",
    "['blockchain'],\n",
    "['smart grid', 'smart city'],\n",
    "['cyber physical'],\n",
    "['internet thing', 'internet ofdashthings', 'iot'],\n",
    "['kinect'],\n",
    "['crowdsourcing'],\n",
    "['facebook', 'twitter', 'social medium', 'tweet'],\n",
    "['energy harvesting'],\n",
    "['device todashdevice', 'd2d'],\n",
    "['massive mimo'],\n",
    "['noma', 'orthogonal multiple'],\n",
    "['edge computing'],\n",
    "['5g', 'fifth generation'],\n",
    "['deep reinforcement learning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457|0.54|9\n",
      "1050|0.6|0\n",
      "3264, 2717|1.04, 0.9|5, 7\n",
      "1617|0.79|5\n",
      "8471, 2263|1.06, 0.62|0, 6\n",
      "2918, 2137|1.15, 0.83|0, 41\n",
      "1999|0.6|0\n",
      "598, 654|0.47, 0.51|3, 13\n",
      "1658, 977|1.08, 0.8|4, 0\n",
      "4939|1.72|5\n",
      "887, 772|0.66, 0.58|4, 1\n",
      "3617, 1247, 799|1.9, 0.98, 0.71|7, 1, 2\n",
      "3508|1.35|0\n",
      "1220, 1799|1.2, 0.57|2, 2\n",
      "1844|0.17|1\n",
      "7812, 867, 9343|0.87, 0.33, 1.16|10, 0, 5\n",
      "796|1.05|0\n",
      "1066|0.75|0\n",
      "1213, 2142, 3595, 1346|0.67, 1.04, 0.76, 0.81|5, 1, 5, 1\n",
      "1199|0.35|18\n",
      "785, 818|0.74, 0.83|3, 4\n",
      "1094|0.64|0\n",
      "1061, 1029|0.59, 0.56|0, 4\n",
      "2499|1.1|6\n",
      "3895, 887|0.96, 0.71|2, 2\n",
      "2012|0.98|0\n"
     ]
    }
   ],
   "source": [
    "for cluster in new_clusters:\n",
    "    max_freq = [stacked_vectors[t].max() for t in cluster]\n",
    "    n2006 = [stacked_vectors[t][2006] for t in cluster]\n",
    "    burstiness = [np.round(10*bd_dataset.burstiness['max'][t],2) for t in cluster]\n",
    "    print(\n",
    "          ', '.join([str(f) for f in max_freq])+'|'+\n",
    "          ', '.join([str(f) for f in burstiness])+'|'+\n",
    "          ', '.join([str(f) for f in n2006])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each cluster, create a time series of mentions in abstracts over time\n",
    "\n",
    "We now need to search for the clusters to pull out the frequency of appearance in abstracts over time. For the cluster [\"Internet of things\", \"IoT\"], all abstracts that mention **either** term are included (i.e. an abstract that uses \"Internet of things\" without the abbreviation \"IoT\" still counts towards the total for that year). We take document frequency, not term frequency, so the number of times the terms are mentioned in each document do not matter, so long as they are mentioned once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|█████▌                                                                             | 1/15 [00:40<09:29, 40.70s/it]\u001b[A\n",
      " 13%|███████████                                                                        | 2/15 [01:25<09:06, 42.08s/it]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 3/15 [02:14<08:48, 44.07s/it]\u001b[A\n",
      " 27%|██████████████████████▏                                                            | 4/15 [03:09<08:39, 47.23s/it]\u001b[A\n",
      " 33%|███████████████████████████▋                                                       | 5/15 [04:09<08:31, 51.11s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 6/15 [05:13<08:13, 54.87s/it]\u001b[A\n",
      " 47%|██████████████████████████████████████▋                                            | 7/15 [06:21<07:50, 58.82s/it]\u001b[A\n",
      " 53%|████████████████████████████████████████████▎                                      | 8/15 [07:35<07:23, 63.43s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 9/15 [09:16<07:27, 74.64s/it]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 10/15 [10:38<06:24, 76.96s/it]\u001b[A\n",
      " 73%|████████████████████████████████████████████████████████████▏                     | 11/15 [12:19<05:36, 84.05s/it]\u001b[A\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 12/15 [14:00<04:27, 89.15s/it]\u001b[A\n",
      " 87%|███████████████████████████████████████████████████████████████████████           | 13/15 [15:49<03:10, 95.09s/it]\u001b[A\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▌     | 14/15 [17:54<01:44, 104.03s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [20:00<00:00, 80.04s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "clusters = pd.read_csv('clusters2.csv')\n",
    "cluster_list = [c.split(', ') for c in clusters['terms']]\n",
    "\n",
    "# List all the cluster terms. This will be more than the total number of clusters.\n",
    "all_cluster_terms = sum(cluster_list,[])\n",
    "\n",
    "# Get the cluster titles. This is the list of terms in each cluster\n",
    "cluster_titles = list(clusters['title'])\n",
    "\n",
    "years = list(range(2006,2021))\n",
    "\n",
    "# This is where we will store the data. The columns correspond to clusters, the rows to years\n",
    "prevalence_array = np.zeros([len(years), len(cluster_list)])\n",
    "    \n",
    "for i, year in enumerate(tqdm(years)):\n",
    "    t0 = time.time()\n",
    "    with open(\"../../Data/semantic_scholar_cleaned_langdetect/\"+str(year)+\".txt\", \"r\") as f:\n",
    "        documents = f.readlines()\n",
    "        documents = [d.strip() for d in documents] \n",
    "        \n",
    "    vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             ngram_range=(1,4),\n",
    "                             vocabulary=all_cluster_terms,\n",
    "                             stop_words=stop\n",
    "                            )\n",
    "    \n",
    "    vector = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    del documents\n",
    "    \n",
    "    for j, cluster in enumerate(cluster_list):\n",
    "        indices = []\n",
    "        for term in cluster:\n",
    "            indices.append(all_cluster_terms.index(term))\n",
    "\n",
    "            # If there are multiple terms in a cluster, sum the cluster columns together\n",
    "            summed_column = np.squeeze(np.asarray(vector[:,indices].sum(axis=1).flatten()))\n",
    "            # Set any element greater than one to one--we're only counting documents here, not \n",
    "            # total occurrences\n",
    "            summed_column[summed_column!=0] = 1\n",
    "\n",
    "            # This is the total number of occurrences of the cluster per year\n",
    "            prevalence_array[i, j] = np.sum(summed_column)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Save the data\n",
    "df = pd.DataFrame(data=prevalence_array, index=years, columns=cluster_titles) \n",
    "pickle.dump(df, open('../cluster_prevalence/papers.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
